{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":39911,"sourceType":"datasetVersion","datasetId":31296},{"sourceId":9001319,"sourceType":"datasetVersion","datasetId":5422325},{"sourceId":9125886,"sourceType":"datasetVersion","datasetId":5509508},{"sourceId":9146514,"sourceType":"datasetVersion","datasetId":5524703},{"sourceId":9256859,"sourceType":"datasetVersion","datasetId":5598152},{"sourceId":10012560,"sourceType":"datasetVersion","datasetId":6164305},{"sourceId":178412,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":151984,"modelId":174438},{"sourceId":178424,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":151994,"modelId":174448},{"sourceId":178445,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":152008,"modelId":174462}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-Tuning CLIP for Swahili Semantic Video Search\r\n\r\n## Introduction\r\n\r\nThe growing adoption of artificial intelligence (AI) in language processing has largely focused on high-resource languages, leaving underrepresented languages like Swahili with limited support. This research addresses this gap by fine-tuning the pre-trained Contrastive Language-Image Pretraining (CLIP) model to handle Swahili queries effectively. Leveraging transfer learning, the study adapts the model to understand and generate meaningful responses to Swahili text. By utilizing a specialized dataset of Swahili text-image pairs, the fine-tuned model is trained to capture the contextual and semantic intricacies of the Swahili language.\r\n\r\nThe primary objective is to develop a robust system capable of performing semantic video searches based on Swahili queries. This advancement aims to enhance accessibility and usability for Swahili-speaking users, fostering inclusivity in AI technologies. Beyond semantic search, the outcomes of this research hold potential applications across education, media, and information retrieval, bridging the language barrier in AI-powered solutions and making such technologies more accessible to diverse linguistic communities.\r\n","metadata":{}},{"cell_type":"code","source":"# !pip install torch torchvision transformers pillow tqdm sentencepiece","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-25T02:31:27.960216Z","iopub.execute_input":"2024-11-25T02:31:27.960863Z","iopub.status.idle":"2024-11-25T02:31:27.964883Z","shell.execute_reply.started":"2024-11-25T02:31:27.960832Z","shell.execute_reply":"2024-11-25T02:31:27.963953Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import MarianMTModel, MarianTokenizer\nfrom torch.utils.data import Dataset, DataLoader, ConcatDataset\nfrom transformers import CLIPProcessor, CLIPModel, CLIPTokenizer, XLMRobertaModel, XLMRobertaTokenizer,AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup\nfrom tqdm import tqdm\nfrom torch.nn import CrossEntropyLoss\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.model_selection import train_test_split\nfrom collections import Counter\nimport numpy as np\nimport pandas as pd\nimport re\nfrom safetensors.torch import load_file\n\n\nsns.set(style=\"darkgrid\")\n\nos.environ[\"XLA_FLAGS\"] = \"--xla_gpu_cuda_data_dir=/usr/local/cuda\"\nos.environ[\"CUDA_MODULE_LOADING\"] = \"LAZY\"\n\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-11-25T19:54:30.232270Z","iopub.execute_input":"2024-11-25T19:54:30.233082Z","iopub.status.idle":"2024-11-25T19:54:30.247864Z","shell.execute_reply.started":"2024-11-25T19:54:30.233046Z","shell.execute_reply":"2024-11-25T19:54:30.246614Z"},"trusted":true},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"## Data Loader\n\n### Dataset Summary\r\n\r\nThis research uses the Swahili scene text detection dataset, consisting of 976 natural scene images annotated at the word level. The dataset captures various real-world environments, such as urban settings and public signage, where Swahili text is commonly found. It reflects the linguistic and cultural diversity of Swahili speakers. The datwasis sourced from the puicbl [GitHub repositorye ry, which includes both images and annotatin\n.\r\n\r\nThe dataset is split into training (700 images) and testing (276 images) sets, ensuring effective model training and evalun\n\natio### Overview of `SwahiliDataset` Class\n\nThe `SwahiliDataset` class is a custom PyTorch dataset designed to handle Swahili text-image pairs for training multimodal models. It loads data from a JSON file containing image paths and associated Swahili text, processes the images and text, and provides functionality for dataset exploration and visualizati\r\nn.\r\n\r\n#### Key Features:\r\n- **Text Preprocessing**: Cleans and processes Swahili text by removing special characters and extra spaces.\r\n- **Data Loading**: Efficiently loads and prepares text and image pairs for model input.\r\n- **Dataset Description**: Provides a summary of dataset statistics including number of items, unique images, and text lengths.\r\n- **Visualization**: Displays random samples of image-text pairs to aid in data inn.rocessing.\r\n","metadata":{}},{"cell_type":"code","source":"class SwahiliDataset(Dataset):\n    def __init__(self, json_path, image_folder, processor):\n        with open(json_path, 'r') as f:\n            self.data = json.load(f)[\"data_list\"]\n        self.image_folder = image_folder\n        self.processor = processor\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def clean_text(self, text):\n        text = text.lower()\n        text = re.sub(r'[^\\w\\s]', '', text) ## Remove special characters\n        text = re.sub(r'\\s+', ' ', text).strip() # Remove extra spaces\n        return text\n    \n    def __getitem__(self, idx):\n        item = self.data[idx]\n        image_path = item['img_path']\n        text = \" \".join([instance[\"text\"] for instance in item[\"instances\"]])\n        text = self.clean_text(text)\n        \n        full_image_path = os.path.join(self.image_folder, image_path)\n        if not os.path.exists(full_image_path):\n            raise FileNotFoundError(f\"Image file not found: {full_image_path}\")\n        \n        image = Image.open(full_image_path).convert(\"RGB\")\n        \n        inputs = self.processor(text=text, images=image, return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n        \n        return text, inputs['input_ids'][0], inputs['pixel_values'][0]\n     ## Function to describe the data\n    \n    def describe(self):\n        num_items = len(self.data)\n\n        first_text_input, input_ids, pixel_values = self.__getitem__(0)\n\n        text_shape = input_ids.shape  # Shape of the input_ids tensor\n        image_shape = pixel_values.shape  # Shape of the pixel_values tensor\n\n        text_lengths = [len(\" \".join([instance[\"text\"] for instance in item[\"instances\"]]).split()) for item in self.data]\n        avg_text_length = np.mean(text_lengths)\n\n        unique_images = set(item['img_path'] for item in self.data)\n        num_unique_images = len(unique_images)\n\n        unique_texts = set(\" \".join([instance[\"text\"] for instance in item[\"instances\"]]) for item in self.data)\n        num_unique_texts = len(unique_texts)\n\n        image_dims = [Image.open(os.path.join(self.image_folder, item['img_path'])).size for item in self.data]\n        image_dims_counter = Counter(image_dims)\n\n        sample_texts = [item[\"instances\"][0][\"text\"] for item in self.data[:5]]\n\n        missing_images = [item['img_path'] for item in self.data if not os.path.exists(os.path.join(self.image_folder, item['img_path']))]\n        num_missing_images = len(missing_images)\n\n        description = {\n            \"Number of items\": num_items,\n            \"Text input shape\": text_shape,\n            \"Image input shape\": image_shape,\n            \"Average text length\": avg_text_length,\n            \"Number of unique images\": num_unique_images,\n            \"Number of unique texts\": num_unique_texts,\n            \"Sample texts\": sample_texts,\n            \"Number of missing images\": num_missing_images,\n        }\n\n        return description\n    \n    def plot_sample(self, num_samples=5):\n        \"\"\"Plot a sample of image-text pairs using Seaborn's aesthetics with Matplotlib.\"\"\"\n        sns.set(style=\"whitegrid\")\n        indices = np.random.choice(len(self.data), num_samples, replace=False)\n        \n        plt.figure(figsize=(15, num_samples * 5))\n        for i, idx in enumerate(indices):\n            item = self.data[idx]\n            image_path = item['img_path']\n            text = \" \".join([instance[\"text\"] for instance in item[\"instances\"]])\n            full_image_path = os.path.join(self.image_folder, image_path)\n            image = Image.open(full_image_path)\n            \n            plt.subplot(num_samples, 1, i + 1)\n            plt.imshow(image)  # Use Matplotlib's imshow to display the image\n            plt.title(text, fontsize=14)\n            plt.axis('off')\n        \n        plt.tight_layout()\n        plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-11-25T18:27:50.820574Z","iopub.execute_input":"2024-11-25T18:27:50.821137Z","iopub.status.idle":"2024-11-25T18:27:50.834737Z","shell.execute_reply.started":"2024-11-25T18:27:50.821108Z","shell.execute_reply":"2024-11-25T18:27:50.833760Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"\"\"\"\nThis function ensures that the data is correctly formatted and ready for processing during training or \nevaluation.\n\"\"\"\ndef collate_fn(batch):\n    texts, input_ids, pixel_values = zip(*batch) \n    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=0)\n    pixel_values = torch.stack(pixel_values)\n    return input_ids, pixel_values\n","metadata":{"execution":{"iopub.status.busy":"2024-11-25T18:27:51.376083Z","iopub.execute_input":"2024-11-25T18:27:51.376926Z","iopub.status.idle":"2024-11-25T18:27:51.381339Z","shell.execute_reply.started":"2024-11-25T18:27:51.376894Z","shell.execute_reply":"2024-11-25T18:27:51.380524Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"### Data Augmentation with Translation\n\nInitial\n\rThe Flickr30k dataset was used to augment the Swahili scene text detection dataset. The dataset consists of more than **31,000 images** sourced from Flickr, each paired with **five descriptive captions** written in English. These captions provide detailed natural language descriptions of the visual content, enabling models to learn the association between images and text.\n\nEnglish comments from the `flickr30k_images` were translated into Kiswahili using the `Helsinki-NLP/opus-mt-en-sw` MarianMT model.  The `translate_to_swahili` function tokenizes the English text, generates the Kiswahili translation, and decodes it into readable text.\r\n","metadata":{}},{"cell_type":"code","source":"flickr30 = pd.read_csv('/kaggle/input/flickr-image-dataset/flickr30k_images/results.csv', delimiter='|')\n\nmodel_name = 'Helsinki-NLP/opus-mt-en-sw'\ntokenizer = MarianTokenizer.from_pretrained(model_name)\nmodel = MarianMTModel.from_pretrained(model_name)\n\ndef translate_to_swahili(text):\n    translated = model.generate(**tokenizer(text, return_tensors=\"pt\", padding=True))\n    return tokenizer.decode(translated[0], skip_special_tokens=True)\n\n# Translate comments\nflickr30['comment_sw'] = flickr30[' comment'].apply(translate_to_swahili)\nflickr30.to_csv('/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_translated.csv', index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class NewSwahiliDataset(Dataset):\n    def __init__(self, csv_path, image_folder, processor):\n        self.data = pd.read_csv(csv_path)\n        self.image_folder = image_folder\n        self.processor = processor\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def clean_text(self, text):\n        text = text.lower()\n        text = re.sub(r'[^\\w\\s]', '', text)  # Remove special characters\n        text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n        return text\n    \n    def __getitem__(self, idx):\n        item = self.data.iloc[idx]\n        image_name = item['image_name']\n        text = item['comment_sw']\n        text = self.clean_text(text)\n        \n        full_image_path = os.path.join(self.image_folder, image_name)\n        if not os.path.exists(full_image_path):\n            raise FileNotFoundError(f\"Image file not found: {full_image_path}\")\n        \n        image = Image.open(full_image_path).convert(\"RGB\")\n        \n        inputs = self.processor(text=text, images=image, return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n        \n        return text, inputs['input_ids'][0], inputs['pixel_values'][0]\n    \n    def describe(self):\n        num_items = len(self.data)\n        \n        # Example of first item in the dataset\n        first_text_input, input_ids, pixel_values = self.__getitem__(0)\n        text_shape = input_ids.shape\n        image_shape = pixel_values.shape\n        \n        # Calculate average text length\n        text_lengths = [len(item.split()) for item in self.data['comment_sw']]\n        avg_text_length = np.mean(text_lengths)\n        \n        # Count unique images and texts\n        num_unique_images = self.data['image_name'].nunique()\n        num_unique_texts = self.data['comment_sw'].nunique()\n        \n        # Image dimensions\n        image_dims = [Image.open(os.path.join(self.image_folder, img_name)).size for img_name in self.data['image_name']]\n        image_dims_counter = Counter(image_dims)\n        \n        # Sample texts\n        sample_texts = self.data['comment_sw'].head(5).tolist()\n        \n        # Missing images\n        missing_images = [img_name for img_name in self.data['image_name'] if not os.path.exists(os.path.join(self.image_folder, img_name))]\n        num_missing_images = len(missing_images)\n        \n        description = {\n            \"Number of items\": num_items,\n            \"Text input shape\": text_shape,\n            \"Image input shape\": image_shape,\n            \"Average text length\": avg_text_length,\n            \"Number of unique images\": num_unique_images,\n            \"Number of unique texts\": num_unique_texts,\n            \"Sample texts\": sample_texts,\n            \"Number of missing images\": num_missing_images,\n            \"Image dimensions frequency\": image_dims_counter,\n        }\n        \n        return description\n    \n    def plot_sample(self, num_samples=5):\n        sns.set(style=\"whitegrid\")\n        indices = np.random.choice(len(self.data), num_samples, replace=False)\n        \n        plt.figure(figsize=(15, num_samples * 5))\n        for i, idx in enumerate(indices):\n            item = self.data.iloc[idx]\n            image_name = item['image_name']\n            text = item['comment_sw']\n            full_image_path = os.path.join(self.image_folder, image_name)\n            image = Image.open(full_image_path)\n            \n            plt.subplot(num_samples, 1, i + 1)\n            plt.imshow(image)  \n            plt.title(text, fontsize=14)\n            plt.axis('off')\n        \n        plt.tight_layout()\n        plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-11-25T18:27:54.690437Z","iopub.execute_input":"2024-11-25T18:27:54.690775Z","iopub.status.idle":"2024-11-25T18:27:54.703637Z","shell.execute_reply.started":"2024-11-25T18:27:54.690751Z","shell.execute_reply":"2024-11-25T18:27:54.702731Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Data Loading\n\n\r\nThisectionde prepares datasets for fine-tunita. It combines two datasets: the original Swahili dataset and a translated Flickr30K dataset, creating a diverse training and testing set. The datasets are preprocessed using a `CLIPProcessor` and loaded into `DataLoader` instances for efficient batching and shuffling during training and evaluation.\r\n","metadata":{}},{"cell_type":"code","source":"processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n# Create datasets and dataloaders\noriginal_train_dataset = SwahiliDataset('/kaggle/input/swahilidata/swahili/train/train.json', \n                               '/kaggle/input/swahilidata/swahili/train/train_imgs',\n                               processor)\n\nnew_train_dataset = NewSwahiliDataset(\n    '/kaggle/input/flick30-swahili/Flick30_translated/flickr30_translated_train.csv',\n    '/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images',\n    processor\n)\n\n# Combine datasets\ncombined_train_dataset = ConcatDataset([original_train_dataset, new_train_dataset])\n\ntrain_dataloader = DataLoader(combined_train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn, num_workers=2, drop_last=True)\n\noriginal_test_dataset = SwahiliDataset('/kaggle/input/swahilidata/swahili/test/test.json', \n                              '/kaggle/input/swahilidata/swahili/test/test_imgs',\n                              processor)\n\nnew_test_dataset = NewSwahiliDataset(\n    '/kaggle/input/flick30-swahili/Flick30_translated/flickr30_translated_test.csv',\n    '/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images',\n    processor\n)\n\ncombined_test_dataset = ConcatDataset([original_test_dataset, new_test_dataset])\n\ntest_dataloader = DataLoader(combined_test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn, num_workers=2, drop_last=True)","metadata":{"execution":{"iopub.status.busy":"2024-11-25T18:27:56.713497Z","iopub.execute_input":"2024-11-25T18:27:56.713825Z","iopub.status.idle":"2024-11-25T18:28:08.817390Z","shell.execute_reply.started":"2024-11-25T18:27:56.713799Z","shell.execute_reply":"2024-11-25T18:28:08.816210Z"},"trusted":true},"outputs":[{"name":"stderr","text":"2024-11-25 18:27:58.314903: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-11-25 18:27:58.315018: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-11-25 18:27:58.444243: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba9efcef8a94478298a675f5b498630b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c547d9c2c0f42e4b02b7b423eee8414"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/862k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6231b18d89814d2e846b6bba63185aac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49a572142c354d1ca93360d52f2ba585"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f73ed61deb147a78349c576ba5dd33c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d924d985dc74aa4840c8261ac077ad1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8da28f196dea43709414441d26de10b4"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"print('Original Train Dataset')\noriginal_train_desc = original_train_dataset.describe()\noriginal_train_df = pd.DataFrame([original_train_desc])\nprint(original_train_df.T)\n\nprint('New Train Dataset')\nnew_train_desc = new_train_dataset.describe()\nnew_train_df = pd.DataFrame([new_train_desc])\nprint(new_train_df.T)\n\nprint('Original Test Dataset')\noriginal_test_desc = original_test_dataset.describe()\noriginal_test_df = pd.DataFrame([original_test_desc])\nprint(original_train_df.T)\n\nprint('New Test Dataset')\nnew_test_desc = new_test_dataset.describe()\nnew_test_df = pd.DataFrame([new_test_desc])\nprint(new_train_df.T)","metadata":{"execution":{"iopub.status.busy":"2024-11-24T00:53:40.396353Z","iopub.execute_input":"2024-11-24T00:53:40.396676Z","iopub.status.idle":"2024-11-24T00:56:56.176736Z","shell.execute_reply.started":"2024-11-24T00:53:40.396651Z","shell.execute_reply":"2024-11-24T00:56:56.175787Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"original_train_dataset.plot_sample()\nnew_train_dataset.plot_sample()","metadata":{"execution":{"iopub.status.busy":"2024-09-19T15:27:00.090283Z","iopub.execute_input":"2024-09-19T15:27:00.091254Z","iopub.status.idle":"2024-09-19T15:27:05.354181Z","shell.execute_reply.started":"2024-09-19T15:27:00.091217Z","shell.execute_reply":"2024-09-19T15:27:05.352767Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## CLIP Fine Tuned","metadata":{}},{"cell_type":"code","source":"# Initialize the CLIP model and processor\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Use Data Parallelism if multiple GPUs are available\nif torch.cuda.device_count() > 1:\n    model = torch.nn.DataParallel(model)\n\nmodel.to(device)\n\n# Training settings\noptimizer = AdamW(model.parameters(), lr=5e-5)\nnum_epochs = 5\n\n# Checkpoint directory and file\ncheckpoint_dir = \"/kaggle/working/checkpoints\"\ncheckpoint_file = os.path.join(checkpoint_dir, \"clip_model_checkpoint.pth\")\nos.makedirs(checkpoint_dir, exist_ok=True)\n\n# Load checkpoint if it exists\nstart_epoch = 0\ntrain_losses = []\ntest_losses = []\nif os.path.isfile(checkpoint_file):\n    checkpoint = torch.load(checkpoint_file)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    start_epoch = checkpoint['epoch'] + 1\n    train_losses = checkpoint['train_losses']\n    test_losses = checkpoint['test_losses']\n    print(f\"Resuming training from epoch {start_epoch}\")\n\n# Training loop\ntry:\n    for epoch in range(start_epoch, num_epochs):\n        model.train()\n        total_loss = 0\n\n        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n            input_ids, pixel_values = batch\n            input_ids = input_ids.to(device)\n            pixel_values = pixel_values.to(device)\n\n            optimizer.zero_grad()\n\n            outputs = model(input_ids=input_ids, pixel_values=pixel_values)\n\n            # Calculate the loss\n            logits_per_image = outputs.logits_per_image\n            logits_per_text = outputs.logits_per_text\n\n            num_classes = logits_per_image.size(1)  # This should be 16 based on the model output\n            labels = torch.arange(len(logits_per_image)) % num_classes  # Ensure labels are within range\n            labels = labels.to(device)\n\n            loss_image = F.cross_entropy(logits_per_image, labels)\n            loss_text = F.cross_entropy(logits_per_text, labels)\n            loss = (loss_image + loss_text) / 2\n\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        avg_loss = total_loss / len(train_dataloader)\n        train_losses.append(avg_loss)\n        print(f\"Epoch {epoch + 1}/{num_epochs}, Average Train Loss: {avg_loss:.4f}\")\n\n        # Evaluation on test set\n        model.eval()\n        test_loss = 0\n        correct = 0\n        total = 0\n\n        with torch.no_grad():\n            total_samples = 0\n            for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n                input_ids, pixel_values = batch\n                input_ids = input_ids.to(device)\n                pixel_values = pixel_values.to(device)\n\n                outputs = model(input_ids=input_ids, pixel_values=pixel_values)\n\n                logits_per_image = outputs.logits_per_image\n                logits_per_text = outputs.logits_per_text\n\n                labels = torch.arange(len(logits_per_image)) % logits_per_image.size(1)\n                labels = labels.to(device)\n\n                loss_image = F.cross_entropy(logits_per_image, labels)\n                loss_text = F.cross_entropy(logits_per_text, labels)\n                loss = (loss_image + loss_text) / 2\n\n                test_loss += loss.item()\n\n                # Calculate accuracy\n                pred = torch.argmax(logits_per_image, dim=1)\n                correct += (pred == labels).sum().item()\n                total += len(labels)\n                total_samples += len(labels)\n\n            if total_samples == 0:\n                print(\"Warning: No samples processed in the evaluation loop. Check your DataLoader.\")\n            else:\n                avg_test_loss = test_loss / len(test_dataloader)\n                test_losses.append(avg_test_loss)\n                accuracy = correct / total\n                print(f\"Test Loss: {avg_test_loss:.4f}, Accuracy: {accuracy:.4f}\")\n\n        # Save checkpoint at the end of each epoch\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'train_losses': train_losses,\n            'test_losses': test_losses\n        }, checkpoint_file)\n        print(f\"Checkpoint saved for epoch {epoch + 1}\")\n\nexcept Exception as e:\n    print(f\"An error occurred during training: {str(e)}\")\n\n# Save the fine-tuned model\nmodel.module.save_pretrained(\"/kaggle/working/fine-tuned-clip-swahili\") if torch.cuda.device_count() > 1 else model.save_pretrained(\"/kaggle/working/fine-tuned-clip-swahili\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T11:42:44.424156Z","iopub.execute_input":"2024-11-23T11:42:44.424994Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training process with 5 epochs takes approximately 3 hours to train on GPU T4 x2 with 16GB RAM each.","metadata":{}},{"cell_type":"code","source":"# Plot the training and testing loss\nplt.figure(figsize=(10, 5))\nplt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss')\nplt.plot(range(1, num_epochs + 1), test_losses, label='Test Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training and Testing Loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T14:32:05.380068Z","iopub.execute_input":"2024-11-23T14:32:05.380434Z","iopub.status.idle":"2024-11-23T14:32:05.733123Z","shell.execute_reply.started":"2024-11-23T14:32:05.380400Z","shell.execute_reply":"2024-11-23T14:32:05.732162Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluate Model","metadata":{}},{"cell_type":"code","source":"def evaluate_model(model, dataloader, is_custom_model=False):\n    model.eval()\n    all_labels = []\n    all_preds = []\n    \n    with torch.no_grad():\n        for batch in tqdm(dataloader, total=len(dataloader)):\n            input_ids, pixel_values = batch\n            input_ids = input_ids.to(device)\n            pixel_values = pixel_values.to(device)\n            \n            if is_custom_model:\n                logits_per_image, _ = model(input_ids=input_ids, pixel_values=pixel_values)\n            else:\n                outputs = model(input_ids=input_ids, pixel_values=pixel_values)\n                logits_per_image = outputs.logits_per_image\n            \n            preds = torch.argmax(logits_per_image, dim=1)\n            labels = torch.arange(len(input_ids), device=device)\n            \n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    accuracy = accuracy_score(all_labels, all_preds)\n    precision = precision_score(all_labels, all_preds, average='weighted')\n    recall = recall_score(all_labels, all_preds, average='weighted')\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    \n    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}","metadata":{"execution":{"iopub.status.busy":"2024-11-25T18:28:13.785978Z","iopub.execute_input":"2024-11-25T18:28:13.786842Z","iopub.status.idle":"2024-11-25T18:28:13.793794Z","shell.execute_reply.started":"2024-11-25T18:28:13.786806Z","shell.execute_reply":"2024-11-25T18:28:13.793038Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"LOCAL_MODEL_DIR = '/kaggle/input/flicke30-clipfinetuned/fine-tuned-clip-swahili'\n\n# Load the fine-tuned model\nmodel = CLIPModel.from_pretrained(LOCAL_MODEL_DIR)\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-11-23T19:03:24.501899Z","iopub.execute_input":"2024-11-23T19:03:24.502215Z","iopub.status.idle":"2024-11-23T19:03:30.582970Z","shell.execute_reply.started":"2024-11-23T19:03:24.502192Z","shell.execute_reply":"2024-11-23T19:03:30.582166Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate the model\nresults_clip_finetuned = evaluate_model(model, test_dataloader)\nprint(\"Evaluation results:\", results_clip_finetuned)","metadata":{"execution":{"iopub.status.busy":"2024-11-23T15:56:57.454010Z","iopub.execute_input":"2024-11-23T15:56:57.454361Z","iopub.status.idle":"2024-11-23T16:00:27.890382Z","shell.execute_reply.started":"2024-11-23T15:56:57.454333Z","shell.execute_reply":"2024-11-23T16:00:27.889399Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Multilingual Finetuned\n\n### Data Loader","metadata":{}},{"cell_type":"markdown","source":"### Multilingual CLIP Model","metadata":{}},{"cell_type":"code","source":"class SwahiliDatasetMultilingual(Dataset):\n    def __init__(self, json_path, image_folder, processor, tokenizer=None):\n        with open(json_path, 'r') as f:\n            self.data = json.load(f)[\"data_list\"]\n        self.image_folder = image_folder\n        self.processor = processor\n        self.tokenizer = tokenizer\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def clean_text(self, text):\n        text = text.lower()\n        text = re.sub(r'[^\\w\\s]', '', text) \n        text = re.sub(r'\\s+', ' ', text).strip() \n        return text\n    \n    def __getitem__(self, idx):\n        item = self.data[idx]\n        image_path = item['img_path']\n        text = \" \".join([instance[\"text\"] for instance in item[\"instances\"]])\n        text = self.clean_text(text)\n        \n        full_image_path = os.path.join(self.image_folder, image_path)\n        if not os.path.exists(full_image_path):\n            raise FileNotFoundError(f\"Image file not found: {full_image_path}\")\n        \n        image = Image.open(full_image_path).convert(\"RGB\")\n        \n        inputs = self.processor(text=text, images=image, return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n        \n        return text, inputs['input_ids'][0], inputs['pixel_values'][0]\n\n   \n    ## Function to describe the data\n    def describe(self):\n        num_items = len(self.data)\n        \n        first_text_input, first_image_input = self.__getitem__(0)\n        text_shape = first_text_input.shape\n        image_shape = first_image_input.shape\n        \n        text_lengths = [len(\" \".join([instance[\"text\"] for instance in item[\"instances\"]]).split()) for item in self.data]\n        avg_text_length = np.mean(text_lengths)\n        \n        unique_images = set(item['img_path'] for item in self.data)\n        num_unique_images = len(unique_images)\n        \n        unique_texts = set(\" \".join([instance[\"text\"] for instance in item[\"instances\"]]) for item in self.data)\n        num_unique_texts = len(unique_texts)\n        \n        image_dims = [Image.open(os.path.join(self.image_folder, item['img_path'])).size for item in self.data]\n        image_dims_counter = Counter(image_dims)\n        \n        sample_texts = [item[\"instances\"][0][\"text\"] for item in self.data[:5]]\n        \n        missing_images = [item['img_path'] for item in self.data if not os.path.exists(os.path.join(self.image_folder, item['img_path']))]\n        num_missing_images = len(missing_images)\n        \n        description = {\n            \"Number of items\": num_items,\n            \"Text input shape\": text_shape,\n            \"Image input shape\": image_shape,\n            \"Average text length\": avg_text_length,\n            \"Number of unique images\": num_unique_images,\n            \"Number of unique texts\": num_unique_texts,\n#             \"Image dimensions frequency\": image_dims_counter,\n            \"Sample texts\": sample_texts,\n            \"Number of missing images\": num_missing_images,\n        }\n        \n        return description\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T19:10:46.420931Z","iopub.execute_input":"2024-11-25T19:10:46.421619Z","iopub.status.idle":"2024-11-25T19:10:46.432887Z","shell.execute_reply.started":"2024-11-25T19:10:46.421589Z","shell.execute_reply":"2024-11-25T19:10:46.431976Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class NewSwahiliDatasetMultilingual(Dataset):\n    def __init__(self, csv_path, image_folder, processor, tokenizer=None):\n        self.data = pd.read_csv(csv_path)\n        self.image_folder = image_folder\n        self.processor = processor\n        self.tokenizer = tokenizer\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def clean_text(self, text):\n        text = text.lower()\n        text = re.sub(r'[^\\w\\s]', '', text)\n        text = re.sub(r'\\s+', ' ', text).strip()\n        return text\n    \n    def __getitem__(self, idx):\n        item = self.data.iloc[idx]\n        image_name = item['image_name']\n        text = item['comment_sw']\n        text = self.clean_text(text)\n        \n        full_image_path = os.path.join(self.image_folder, image_name)\n        if not os.path.exists(full_image_path):\n            raise FileNotFoundError(f\"Image file not found: {full_image_path}\")\n        \n        image = Image.open(full_image_path).convert(\"RGB\")\n        \n        inputs = self.processor(text=text, images=image, return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n        \n        return text, inputs['input_ids'][0], inputs['pixel_values'][0]\n    \n    def describe(self):\n        num_items = len(self.data)\n        first_text_input, input_ids, pixel_values = self.__getitem__(0)\n        text_shape = input_ids.shape\n        image_shape = pixel_values.shape\n        text_lengths = [len(item.split()) for item in self.data['comment_sw']]\n        avg_text_length = np.mean(text_lengths)\n        num_unique_images = self.data['image_name'].nunique()\n        num_unique_texts = self.data['comment_sw'].nunique()\n        image_dims = [Image.open(os.path.join(self.image_folder, img_name)).size for img_name in self.data['image_name']]\n        image_dims_counter = Counter(image_dims)\n        sample_texts = self.data['comment_sw'].head(5).tolist()\n        missing_images = [img_name for img_name in self.data['image_name'] if not os.path.exists(os.path.join(self.image_folder, img_name))]\n        num_missing_images = len(missing_images)\n        \n        description = {\n            \"Number of items\": num_items,\n            \"Text input shape\": text_shape,\n            \"Image input shape\": image_shape,\n            \"Average text length\": avg_text_length,\n            \"Number of unique images\": num_unique_images,\n            \"Number of unique texts\": num_unique_texts,\n            \"Sample texts\": sample_texts,\n            \"Number of missing images\": num_missing_images,\n            \"Image dimensions frequency\": image_dims_counter,\n        }\n        \n        return description\n    \n    def plot_sample(self, num_samples=5):\n        sns.set(style=\"whitegrid\")\n        indices = np.random.choice(len(self.data), num_samples, replace=False)\n        plt.figure(figsize=(15, num_samples * 5))\n        for i, idx in enumerate(indices):\n            item = self.data.iloc[idx]\n            image_name = item['image_name']\n            text = item['comment_sw']\n            full_image_path = os.path.join(self.image_folder, image_name)\n            image = Image.open(full_image_path)\n            plt.subplot(num_samples, 1, i + 1)\n            plt.imshow(image)\n            plt.title(text, fontsize=14)\n            plt.axis('off')\n        plt.tight_layout()\n        plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T19:10:47.213330Z","iopub.execute_input":"2024-11-25T19:10:47.213965Z","iopub.status.idle":"2024-11-25T19:10:47.226945Z","shell.execute_reply.started":"2024-11-25T19:10:47.213936Z","shell.execute_reply":"2024-11-25T19:10:47.226041Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"from torch.utils.data import DataLoader, ConcatDataset\nfrom transformers import CLIPProcessor, CLIPTokenizer\n\n# Initialize the processor and tokenizer\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\ntokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n# Create the original and new multilingual datasets\noriginal_train_dataset = SwahiliDatasetMultilingual(\n    '/kaggle/input/swahilidata/swahili/train/train.json', \n    '/kaggle/input/swahilidata/swahili/train/train_imgs',\n    processor, tokenizer\n)\n\nnew_train_dataset = NewSwahiliDatasetMultilingual(\n    '/kaggle/input/flick30-swahili/Flick30_translated/flickr30_translated_train.csv',\n    '/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images',\n    processor\n)\n\n# Combine the datasets for training\ncombined_train_dataset = ConcatDataset([original_train_dataset, new_train_dataset])\n\n# DataLoader for the combined train dataset\ntrain_dataloader = DataLoader(\n    combined_train_dataset, \n    batch_size=32, \n    shuffle=True, \n    collate_fn=collate_fn, \n    num_workers=2, \n    drop_last=True\n)\n\n# Create the original and new multilingual test datasets\noriginal_test_dataset = SwahiliDatasetMultilingual(\n    '/kaggle/input/swahilidata/swahili/test/test.json', \n    '/kaggle/input/swahilidata/swahili/test/test_imgs',\n    processor, tokenizer\n)\n\nnew_test_dataset = NewSwahiliDatasetMultilingual(\n    '/kaggle/input/flick30-swahili/Flick30_translated/flickr30_translated_test.csv',\n    '/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images',\n    processor\n)\n\n# Combine the datasets for testing\ncombined_test_dataset = ConcatDataset([original_test_dataset, new_test_dataset])\n\n# DataLoader for the combined test dataset\ntest_dataloader = DataLoader(\n    combined_test_dataset, \n    batch_size=32, \n    shuffle=False, \n    collate_fn=collate_fn, \n    num_workers=2, \n    drop_last=True\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-25T19:10:47.989369Z","iopub.execute_input":"2024-11-25T19:10:47.989712Z","iopub.status.idle":"2024-11-25T19:10:49.061044Z","shell.execute_reply.started":"2024-11-25T19:10:47.989685Z","shell.execute_reply":"2024-11-25T19:10:49.060270Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class CustomMultilingualCLIPModel(nn.Module):\n    def __init__(self, clip_model, xlm_model):\n        super().__init__()\n        self.clip_model = clip_model\n        self.xlm_model = xlm_model\n        self.projection = nn.Linear(xlm_model.config.hidden_size, clip_model.text_model.config.hidden_size)\n\n    def forward(self, input_ids, pixel_values, attention_mask=None):\n        xlm_outputs = self.xlm_model(input_ids=input_ids, attention_mask=attention_mask)\n        text_features = self.projection(xlm_outputs.last_hidden_state[:, 0, :])\n        image_features = self.clip_model.get_image_features(pixel_values=pixel_values)\n        \n        # Normalize features\n        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n\n        # Compute similarity\n        logits_per_image = torch.matmul(image_features, text_features.t()) * self.clip_model.logit_scale.exp()\n        logits_per_text = logits_per_image.t()\n\n        return logits_per_image, logits_per_text\n\n    def save(self, save_directory):\n        if not os.path.exists(save_directory):\n            os.makedirs(save_directory)\n        torch.save(self.state_dict(), os.path.join(save_directory, \"custom_clip_multilingual_model.pth\"))\n        self.clip_model.save_pretrained(save_directory)\n        self.xlm_model.save_pretrained(save_directory)","metadata":{"execution":{"iopub.status.busy":"2024-11-25T19:10:49.566286Z","iopub.execute_input":"2024-11-25T19:10:49.567320Z","iopub.status.idle":"2024-11-25T19:10:49.574863Z","shell.execute_reply.started":"2024-11-25T19:10:49.567261Z","shell.execute_reply":"2024-11-25T19:10:49.574087Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def train_model(model, train_dataloader, test_dataloader, num_epochs, checkpoint_dir=\"/kaggle/working/checkpoints\"):\n    # Training settings\n    optimizer = AdamW(model.parameters(), lr=5e-5)\n    loss_fn = CrossEntropyLoss()\n\n    # Checkpoint directory and file\n    os.makedirs(checkpoint_dir, exist_ok=True)\n    checkpoint_file = os.path.join(checkpoint_dir, \"clip_model_checkpoint.pth\")\n\n    # Load checkpoint if it exists\n    start_epoch = 0\n    train_losses = []\n    test_losses = []\n    if os.path.isfile(checkpoint_file):\n        checkpoint = torch.load(checkpoint_file)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        start_epoch = checkpoint['epoch'] + 1\n        train_losses = checkpoint['train_losses']\n        test_losses = checkpoint['test_losses']\n        print(f\"Resuming training from epoch {start_epoch}\")\n\n    for epoch in range(start_epoch, num_epochs):\n        model.train()\n        train_loss = 0.0\n        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n            optimizer.zero_grad()\n            \n            input_ids, pixel_values = batch\n            input_ids = input_ids.to(device)\n            pixel_values = pixel_values.to(device)\n            \n            logits_per_image, logits_per_text = model(input_ids=input_ids, pixel_values=pixel_values)\n            ground_truth = torch.arange(len(input_ids), dtype=torch.long, device=device)\n            \n            total_loss = (loss_fn(logits_per_image, ground_truth) + loss_fn(logits_per_text, ground_truth)) / 2\n            total_loss.backward()\n            optimizer.step()\n            \n            train_loss += total_loss.item()\n        \n        avg_train_loss = train_loss / len(train_dataloader)\n        train_losses.append(avg_train_loss)\n        print(f\"Epoch {epoch + 1}/{num_epochs}, Average Train Loss: {avg_train_loss:.4f}\")\n\n        # Evaluation\n        model.eval()\n        test_loss = 0.0\n        correct_predictions = 0\n        total_predictions = 0\n        with torch.no_grad():\n            for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n                input_ids, pixel_values = batch\n                input_ids = input_ids.to(device)\n                pixel_values = pixel_values.to(device)\n\n                logits_per_image, logits_per_text = model(input_ids=input_ids, pixel_values=pixel_values)\n                ground_truth = torch.arange(len(input_ids), dtype=torch.long, device=device)\n                \n                total_loss = (loss_fn(logits_per_image, ground_truth) + loss_fn(logits_per_text, ground_truth)) / 2\n                test_loss += total_loss.item()\n                \n                _, predicted = logits_per_text.max(1)\n                correct_predictions += (predicted == ground_truth).sum().item()\n                total_predictions += ground_truth.size(0)\n\n        avg_test_loss = test_loss / len(test_dataloader)\n        test_accuracy = correct_predictions / total_predictions\n        test_losses.append(avg_test_loss)\n        print(f\"Test Loss: {avg_test_loss:.4f}, Accuracy: {test_accuracy:.4f}\")\n\n        # Save checkpoint\n        checkpoint = {\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'train_losses': train_losses,\n            'test_losses': test_losses\n        }\n        torch.save(checkpoint, checkpoint_file)\n        print(f\"Checkpoint saved for epoch {epoch + 1}\")\n\n    return train_losses, test_losses","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T02:32:56.822278Z","iopub.execute_input":"2024-11-25T02:32:56.822937Z","iopub.status.idle":"2024-11-25T02:32:56.835529Z","shell.execute_reply.started":"2024-11-25T02:32:56.822908Z","shell.execute_reply":"2024-11-25T02:32:56.834404Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Swahili Finetuned Model\n\nThe integration of SwahBERT, a Swahili-specific pre-trained language model, into the CLIP framework replaces the original text encoder to enhance the model's understanding of Swahili text. SwahBERT, which shares a 12-layer transformer architecture similar to CLIP's text encoder, serves as a compatible replacement. This allows the model to better process and relate Swahili text to corresponding visual content.\n\nTo align the output dimensions of the text encoder with those of the vision encoder, a linear projection layer is employed. This alignment enables the computation of dot-product similarity in a shared embedding space, critical for effective cross-modal representations.\n\nThe training process incorporates transfer learning techniques to bridge the gap between the source (generalized CLIP) and target (Swahili-specific) domains. Using a custom model architecture that combines CLIP's vision encoder and SwahBERT, the model is trained on paired data for five epochs. The training process monitors both training and testing losses to fine-tune the model while preserving its ability to generalize. This approach optimizes the joint embedding space for improved text-image relationships in Swahili.","metadata":{}},{"cell_type":"code","source":"# Load model \n\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nswahbert = AutoModel.from_pretrained(\"orai-nlp/bert-base-sw\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmultilingual_model_sb = CustomMultilingualCLIPModel(model, swahbert)\nmultilingual_model_sb.to(device)\n\n# Train the model\nnum_epochs = 5\ntrain_losses, test_losses = train_model(multilingual_model_sb, train_dataloader, test_dataloader, num_epochs)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T07:29:42.407115Z","iopub.execute_input":"2024-11-25T07:29:42.407435Z","iopub.status.idle":"2024-11-25T07:29:47.226351Z","shell.execute_reply.started":"2024-11-25T07:29:42.407412Z","shell.execute_reply":"2024-11-25T07:29:47.225436Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\nfolder_to_zip = \"/kaggle/working/fine-tuned-clip-swahili-multilingual_swahbertV2\" \noutput_zip_file = \"/kaggle/working/fine-tuned-clip-swahili-multilingual_swahbertV2\"\n\n# Create a zip file\nshutil.make_archive(output_zip_file, 'zip', folder_to_zip)\n\nprint(f\"Folder {folder_to_zip} has been compressed into {output_zip_file}.zip\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T07:35:39.289525Z","iopub.execute_input":"2024-11-25T07:35:39.290038Z","iopub.status.idle":"2024-11-25T07:37:28.595177Z","shell.execute_reply.started":"2024-11-25T07:35:39.289996Z","shell.execute_reply":"2024-11-25T07:37:28.594273Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the model\nmultilingual_model_sb.save(\"/kaggle/working/fine-tuned-clip-swahili-multilingual_swahbertV2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T07:31:55.536356Z","iopub.execute_input":"2024-11-25T07:31:55.537039Z","iopub.status.idle":"2024-11-25T07:31:59.972361Z","shell.execute_reply.started":"2024-11-25T07:31:55.537009Z","shell.execute_reply":"2024-11-25T07:31:59.971382Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(train_losses)\nprint(test_losses)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T07:46:56.312705Z","iopub.execute_input":"2024-11-25T07:46:56.313015Z","iopub.status.idle":"2024-11-25T07:46:56.317701Z","shell.execute_reply.started":"2024-11-25T07:46:56.312987Z","shell.execute_reply":"2024-11-25T07:46:56.316823Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot training and testing loss\nplt.figure(figsize=(10, 5))\nplt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss')\nplt.plot(range(1, num_epochs + 1), test_losses, label='Test Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training and Testing Loss SwahiliBert Clip Model')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-25T07:42:06.678393Z","iopub.execute_input":"2024-11-25T07:42:06.679230Z","iopub.status.idle":"2024-11-25T07:42:07.077322Z","shell.execute_reply.started":"2024-11-25T07:42:06.679203Z","shell.execute_reply":"2024-11-25T07:42:07.076521Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate the model\nresults_multilingual_sb = evaluate_model(multilingual_model_sb, test_dataloader,is_custom_model=True)\nprint(\"Evaluation results:\", results_multilingual_sb)","metadata":{"execution":{"iopub.status.busy":"2024-11-25T07:42:59.925252Z","iopub.execute_input":"2024-11-25T07:42:59.925957Z","iopub.status.idle":"2024-11-25T07:46:56.310907Z","shell.execute_reply.started":"2024-11-25T07:42:59.925928Z","shell.execute_reply":"2024-11-25T07:46:56.309820Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Optimization\n\nThe train_model function is designed to efficiently train a PyTorch model while monitoring its performance on both training and testing datasets. It leverages the AdamW optimizer for weight updates and a linear learning rate scheduler to adjust the learning rate dynamically during training. The function calculates the cross-entropy loss for both image and text logits and uses their average as the total loss. To maintain stability, it applies gradient clipping, preventing exploding gradients during backpropagation.\n\nAfter each epoch, the model's performance is evaluated on a test dataset by calculating the average test loss. To ensure robust training and avoid overfitting, the function incorporates an early stopping mechanism. This stops training if there is no improvement in the test loss for a specified number of consecutive epochs. Additionally, the function saves the model's state whenever it achieves the lowest test loss, preserving the best-performing checkpoint.","metadata":{}},{"cell_type":"code","source":"def train_model(model, train_dataloader, test_dataloader, num_epochs):\n    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5, betas=(0.9, 0.98), eps=1e-6, weight_decay=0.2)\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * num_epochs)\n\n    loss_fn = nn.CrossEntropyLoss()\n    train_losses = []\n    test_losses = []\n\n    best_test_loss = float('inf')\n    patience_counter = 0\n    early_stopping_patience = 3\n\n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0.0\n        for batch in tqdm(train_dataloader, total=len(train_dataloader)):\n            optimizer.zero_grad()\n            input_ids, pixel_values = batch\n            input_ids = input_ids.to(device)\n            pixel_values = pixel_values.to(device)\n\n            logits_per_image, logits_per_text = model(input_ids=input_ids, pixel_values=pixel_values)\n            ground_truth = torch.arange(len(input_ids), dtype=torch.long, device=device)\n            total_loss = (loss_fn(logits_per_image, ground_truth) + loss_fn(logits_per_text, ground_truth)) / 2\n\n            total_loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n            scheduler.step()\n\n            train_loss += total_loss.item()\n\n        avg_train_loss = train_loss / len(train_dataloader)\n        train_losses.append(avg_train_loss)\n        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}\")\n\n        model.eval()\n        test_loss = 0.0\n        with torch.no_grad():\n            for batch in tqdm(test_dataloader, total=len(test_dataloader)):\n                input_ids, pixel_values = batch\n                input_ids = input_ids.to(device)\n                pixel_values = pixel_values.to(device)\n\n                logits_per_image, logits_per_text = model(input_ids=input_ids, pixel_values=pixel_values)\n                ground_truth = torch.arange(len(input_ids), dtype=torch.long, device=device)\n                total_loss = (loss_fn(logits_per_image, ground_truth) + loss_fn(logits_per_text, ground_truth)) / 2\n\n                test_loss += total_loss.item()\n\n        avg_test_loss = test_loss / len(test_dataloader)\n        test_losses.append(avg_test_loss)\n        print(f\"Epoch {epoch+1}/{num_epochs}, Test Loss: {avg_test_loss:.4f}\")\n\n        if avg_test_loss < best_test_loss:\n            best_test_loss = avg_test_loss\n            patience_counter = 0\n            # Save the best model\n            torch.save(model.state_dict(), \"best_model.pth\")\n        else:\n            patience_counter += 1\n            if patience_counter >= early_stopping_patience:\n                print(\"Early stopping\")\n                break\n\n    return train_losses, test_losses\n","metadata":{"execution":{"iopub.status.busy":"2024-11-24T09:58:26.327627Z","iopub.execute_input":"2024-11-24T09:58:26.327988Z","iopub.status.idle":"2024-11-24T09:58:26.339143Z","shell.execute_reply.started":"2024-11-24T09:58:26.327955Z","shell.execute_reply":"2024-11-24T09:58:26.338399Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load models\nclip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nswahbert = AutoModel.from_pretrained(\"orai-nlp/bert-base-sw\")\n\n# Update the CustomMultilingualCLIPModel to use SwahBert\nmultilingual_model_sb_fr = CustomMultilingualCLIPModel(clip_model, swahbert)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmultilingual_model_sb_fr.to(device)\n\n# Update the CustomMultilingualCLIPModel to use SwahBert\n# Freezing and unfreezing vision model layers\nvision_model = multilingual_model_sb_fr.clip_model.vision_model\nfor param in vision_model.parameters():\n    param.requires_grad = False\nfor param in vision_model.encoder.layers[-1].parameters():\n    param.requires_grad = True\n\n# Freezing and unfreezing text model layers\ntext_model = multilingual_model_sb_fr.clip_model.text_model\nfor param in text_model.parameters():\n    param.requires_grad = False\nfor param in text_model.encoder.layers[-1].parameters():\n    param.requires_grad = True\n\n# Freezing and unfreezing BERT model layers\nfor param in multilingual_model_sb_fr.xlm_model.parameters():\n    param.requires_grad = False\nfor param in multilingual_model_sb_fr.xlm_model.encoder.layer[-1].parameters():\n    param.requires_grad = True\n\n# Train the model\nnum_epochs = 5\ntrain_losses, test_losses = train_model(multilingual_model_sb_fr, train_dataloader, test_dataloader, num_epochs)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-24T09:58:36.527568Z","iopub.execute_input":"2024-11-24T09:58:36.527949Z","iopub.status.idle":"2024-11-24T11:53:33.789874Z","shell.execute_reply.started":"2024-11-24T09:58:36.527919Z","shell.execute_reply":"2024-11-24T11:53:33.788971Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot training and testing loss\nplt.figure(figsize=(10, 5))\nplt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss')\nplt.plot(range(1, num_epochs + 1), test_losses, label='Test Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training and Testing Loss SwahiliBert Freezing Layers Model')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-24T12:17:07.744186Z","iopub.execute_input":"2024-11-24T12:17:07.744865Z","iopub.status.idle":"2024-11-24T12:17:08.180052Z","shell.execute_reply.started":"2024-11-24T12:17:07.744836Z","shell.execute_reply":"2024-11-24T12:17:08.179234Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate the model\nresults_multilingual_sb_fr = evaluate_model(multilingual_model_sb_fr, test_dataloader,is_custom_model=True)\nprint(\"Evaluation results:\", results_multilingual_sb_fr)","metadata":{"execution":{"iopub.status.busy":"2024-11-24T12:18:41.635833Z","iopub.execute_input":"2024-11-24T12:18:41.636184Z","iopub.status.idle":"2024-11-24T12:22:46.669054Z","shell.execute_reply.started":"2024-11-24T12:18:41.636157Z","shell.execute_reply":"2024-11-24T12:22:46.668030Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"multilingual_model_sb_fr.save(\"/kaggle/working/fine-tuned-clip-swahbert\")","metadata":{"execution":{"iopub.status.busy":"2024-11-24T12:22:55.452839Z","iopub.execute_input":"2024-11-24T12:22:55.453433Z","iopub.status.idle":"2024-11-24T12:23:00.024088Z","shell.execute_reply.started":"2024-11-24T12:22:55.453401Z","shell.execute_reply":"2024-11-24T12:23:00.023088Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Compare Models","metadata":{}},{"cell_type":"code","source":"# Basic CLIP model\nclip_model_basic = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\nresults_basic_clip = evaluate_model(clip_model_basic, test_dataloader, is_custom_model=False)\nprint(\"Basic CLIP model results:\", results_basic_clip)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T20:21:36.918822Z","iopub.execute_input":"2024-11-25T20:21:36.919687Z","iopub.status.idle":"2024-11-25T20:25:25.184662Z","shell.execute_reply.started":"2024-11-25T20:21:36.919650Z","shell.execute_reply":"2024-11-25T20:25:25.183309Z"}},"outputs":[{"name":"stderr","text":"  0%|          | 0/1000 [00:00<?, ?it/s]/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n100%|| 1000/1000 [03:47<00:00,  4.72it/s]/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n100%|| 1000/1000 [03:47<00:00,  4.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Basic CLIP model results: {'accuracy': 0.0898125, 'precision': 0.09006087909131695, 'recall': 0.0898125, 'f1': 0.08981708618954141}\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"LOCAL_MODEL_DIR = '/kaggle/input/flicke30-clipfinetuned/fine-tuned-clip-swahili'\n\n# Load the fine-tuned model\nmodel = CLIPModel.from_pretrained(LOCAL_MODEL_DIR)\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nresults_clip_finetuned = evaluate_model(model, test_dataloader, is_custom_model=False)\nprint(\"CLIP Finetuned model results:\", results_clip_finetuned)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T18:29:24.309847Z","iopub.execute_input":"2024-11-25T18:29:24.310593Z","iopub.status.idle":"2024-11-25T18:33:54.835252Z","shell.execute_reply.started":"2024-11-25T18:29:24.310565Z","shell.execute_reply":"2024-11-25T18:33:54.834164Z"}},"outputs":[{"name":"stderr","text":"  0%|          | 0/1000 [00:00<?, ?it/s]/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n100%|| 1000/1000 [04:23<00:00,  4.72it/s]/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n100%|| 1000/1000 [04:23<00:00,  3.79it/s]","output_type":"stream"},{"name":"stdout","text":"CLIP Finetuned model results: {'accuracy': 0.7148125, 'precision': 0.715094832980223, 'recall': 0.7148125, 'f1': 0.7148523439625032}\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"model_dir = \"/kaggle/input/swah_clip/pytorch/default/1/kaggle/working/fine-tuned-clip-swahili\"\n\nloaded_model = CLIPModel.from_pretrained(model_dir, torch_dtype=\"auto\")\nloaded_model.to(device)\n\nresults_swahclip_model = evaluate_model(loaded_model, test_dataloader, is_custom_model=False)\nprint(\"SwahClip model results:\", results_swahclip_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T20:08:15.789222Z","iopub.execute_input":"2024-11-25T20:08:15.790181Z","iopub.status.idle":"2024-11-25T20:12:05.140535Z","shell.execute_reply.started":"2024-11-25T20:08:15.790144Z","shell.execute_reply":"2024-11-25T20:12:05.139347Z"}},"outputs":[{"name":"stderr","text":"  0%|          | 0/1000 [00:00<?, ?it/s]/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n100%|| 1000/1000 [03:48<00:00,  4.90it/s]/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n100%|| 1000/1000 [03:48<00:00,  4.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"SwahClip model results: {'accuracy': 0.774375, 'precision': 0.7744950322758518, 'recall': 0.774375, 'f1': 0.7743732849942002}\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# Create a DataFrame to store the results\nresults_df = pd.DataFrame({\n    \"Model\": [\"Basic CLIP\", \"Fine-tuned CLIP\", \"SwahCLIP Model Optimized\"],\n    \"Accuracy\": [results_basic_clip[\"accuracy\"], results_clip_finetuned[\"accuracy\"], \n                 results_swahclip_model[\"accuracy\"]],\n    \"Precision\": [results_basic_clip[\"precision\"], results_clip_finetuned[\"precision\"], \n                  results_swahclip_model[\"precision\"]],\n    \"Recall\": [results_basic_clip[\"recall\"], results_clip_finetuned[\"recall\"], \n               results_swahclip_model[\"recall\"]],\n    \"F1-Score\": [results_basic_clip[\"f1\"], results_clip_finetuned[\"f1\"], results_swahclip_model[\"f1\"]]\n})\n\n# Print the results DataFrame\nprint(results_df)\n\n# Plot the results\nplt.figure(figsize=(12, 6))\nresults_df.set_index(\"Model\").plot(kind=\"bar\", figsize=(12, 6))\nplt.title(\"Comparison of Model Performance\")\nplt.ylabel(\"Scores\")\nplt.xticks(rotation=0)\nplt.legend(loc=\"upper right\")\nplt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-25T20:27:56.365582Z","iopub.execute_input":"2024-11-25T20:27:56.366632Z","iopub.status.idle":"2024-11-25T20:27:56.738420Z","shell.execute_reply.started":"2024-11-25T20:27:56.366596Z","shell.execute_reply":"2024-11-25T20:27:56.737380Z"},"trusted":true},"outputs":[{"name":"stdout","text":"                      Model  Accuracy  Precision    Recall  F1-Score\n0                Basic CLIP  0.089813   0.090061  0.089813  0.089817\n1           Fine-tuned CLIP  0.714812   0.715095  0.714812  0.714852\n2  SwahCLIP Model Optimized  0.774375   0.774495  0.774375  0.774373\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x600 with 0 Axes>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x600 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAA/MAAAIsCAYAAAC+4/ntAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB/2klEQVR4nOzdeVxUZf//8fcMiwg4IIq7pmigqbgvqLlWZmmWWeKKWoaFudU3tbxN09LUtMRyNw1LW1xyKZVWc82l1bqtxNxFRWRwQWBmfn/4Y25HQAFZZvT1fDx65FxzznU+s53hPee6zjHYbDabAAAAAACAyzAWdQEAAAAAACB3CPMAAAAAALgYwjwAAAAAAC6GMA8AAAAAgIshzAMAAAAA4GII8wAAAAAAuBjCPAAAAAAALoYwDwAAAACAiyHMAwAAAADgYgjzAADkQEhIiKKjo4u6jFu2Zs0aPfjgg6pdu7YaN25c1OVkcuzYMYWEhGjVqlW5XnfXrl0KCQnRrl27CqCyvFu4cKE6dOigWrVqqWvXrkVdDgDgNuFe1AUAAFzDkSNHtHDhQm3btk2nT5+Wh4eHgoOD1alTJ/Xo0UNeXl5FXSJu4uDBgxozZozuvfdePfPMMzd8zaKjozV79mwZDAZ9++23Kl++vMP9Fy5cUIsWLXTlyhX17t1b48aNK+jy882qVas0ZswY+21PT09VqFBBLVu21HPPPafSpUvn27a2bt2qadOm6ZFHHtHzzz+vkiVL5lvfAIA7G2EeAHBT3333nYYNGyZPT0917dpVwcHBSktL0969ezVt2jT9888/mjhxYlGXWaB+/fVXubm5FXUZt+THH3+U1WrVK6+8orvuuitH63h6emr9+vUaNGiQQ/vmzZsLosRCNXToUFWqVEmpqanau3evli9fru+//17r169X8eLF82UbO3fulNFo1Ouvvy5PT8986RMAAIkwDwC4iaNHj2rEiBGqUKGCli5dqjJlytjv6927tw4fPqzvvvuu6AosQFarVWlpaSpWrJiKFStW1OXcsoSEBElSiRIlcrxOmzZttGHDhkxhfv369Wrbtq02bdqUrzUWptatW6tu3bqSpCeeeEL+/v56//339fXXX6tz58631Pfly5dVvHhxJSQkyMvLK9+CvM1m05UrVxgJAwBgzjwA4MYWLlyoS5cu6fXXX3cI8hnuuusuRURE2G+np6fr3Xff1X333ac6deqoffv2mjFjhlJTUx3Wa9++vSIjI7Vr1y5169ZNoaGh6tKli32+8+bNm9WlSxfVrVtX3bp10x9//OGw/ujRo9WgQQMdPXpUTz31lOrXr69WrVpp9uzZstlsDssuWrRI4eHhatasmUJDQ9WtWzdt3Lgx02MJCQnRa6+9prVr1+rhhx9W3bp19cMPP9jvu3bO/IULF/T666+rffv2qlOnjsLCwjRgwADt37/foc8vv/zS/viaNWumF198UfHx8Vk+lvj4eD333HNq0KCBmjdvrjfffFMWiyXb1+ZaH374oR5++GHVqVNHrVq10oQJE2Q2mx2e74z6w8LCcnwOgM6dO+vPP//UwYMH7W1nzpzRzp07sw28CQkJevnll9WiRQvVrVtXjzzyiFavXp1pObPZrNGjR6tRo0Zq3LixRo0apeTk5Cz7PHjwoIYOHaqmTZva3xNff/31TevPjebNm0u6Om8/w+eff25//Zo2baoRI0bo5MmTDuv17dtXnTt31u+//67evXurXr16mjFjhn3u/6VLlxQSEuJwLoDcfk5++OEHex0rVqywnx/giy++0OzZs3XvvfeqQYMGGjp0qJKTk5WamqrXX39dYWFhatCggcaMGZOp75UrV6pfv34KCwtTnTp19NBDD+mjjz7K9Lxk1LBnzx51795ddevWVYcOHbRmzZpMy5rNZr3xxhv2z0Xr1q310ksv6dy5c/ZlUlNTNWvWLN1///2qU6eO2rRpo6lTp2aqDwBwYxyZBwDc0LfffqvKlSurYcOGOVp+7NixWr16tTp27KgBAwbo119/1bx583Tw4EG9++67DssePnxYL7zwgsLDw/XII49o8eLFGjx4sCZMmKCZM2eqZ8+ekqT58+dr+PDh2rhxo4zG//0ObbFY9PTTT6tevXr6v//7P/3www+Kjo6WxWLRsGHD7Mt98MEHat++vbp06aK0tDRt2LBBw4YN07x589S2bVuHmnbu3Kkvv/xSvXv3VsmSJVWxYsUsH+err76qTZs2qU+fPqpevbrOnz+vvXv36uDBg6pdu7ak/83Nrlu3rkaOHKmEhAR98MEH2rdvn9asWSOTyeTwWJ566imFhobqpZde0o4dO7R48WJVrlxZvXr1uuFznjG/vUWLFurZs6cOHTqk5cuX67ffftPy5cvl4eGhl19+WWvWrFFsbKzGjx8vb29vhYSE3PT1bNKkicqVK6f169fbn9MvvvhC3t7emZ47SUpJSVHfvn115MgR9e7dW5UqVdLGjRs1evRomc1m+w8/NptNzz33nPbu3avw8HBVr15dsbGxGjVqVKY+//77b/Xs2VNly5bVoEGD5O3trS+//FJRUVGKjo7W/ffff9PHkRNHjhyRJPn7+0uS5syZo3feeUedOnVS9+7dde7cOS1btky9e/fO9PqdP39egwYN0sMPP6xHHnlEpUqVUp06dfTJJ5/o119/1aRJkyTJ/jnKzefk0KFDeuGFF9SjRw89+eSTqlatmv2++fPny8vLS88884wOHz6sZcuWyd3dXQaDQWazWUOGDNEvv/yiVatWqWLFihoyZIh93eXLl+vuu+9W+/bt5e7urm+//VYTJkyQzWZT7969HWo4fPiwhg0bpu7du+uxxx7TypUrNXr0aNWuXVt33323JOnixYvq3bu3Dh48qMcff1z33HOPEhMT9c033yg+Pl4BAQGyWq169tlntXfvXj355JOqXr26/vrrLy1dulT//vuv3nvvvXx5LQHgjmADACAbycnJtuDgYNuzzz6bo+X//PNPW3BwsO2VV15xaJ8yZYotODjYtmPHDntbu3btbMHBwbZ9+/bZ23744QdbcHCwLTQ01Hb8+HF7+4oVK2zBwcG2nTt32ttGjRplCw4Otk2cONHeZrVabc8884ytdu3atoSEBHv75cuXHepJTU21de7c2davXz+H9uDgYFvNmjVtf//9d6bHFhwcbJs1a5b9dqNGjWwTJkzI9rlITU21hYWF2Tp37mxLSUmxt3/77be24OBg2zvvvJPpscyePduhj0cffdT22GOPZbsNm81mS0hIsNWuXds2cOBAm8VisbcvW7bMFhwcbPvss8/sbbNmzbIFBwc7PDfZuXbZKVOm2O6//377fY8//rht9OjRNpvt6vNy7fOwZMkSW3BwsO3zzz93eC569Ohhq1+/vi05Odlms9lssbGxtuDgYNuCBQvsy6Wnp9t69eplCw4Otq1cudLeHhERYevcubPtypUr9jar1Wrr0aOH7YEHHrC37dy5M9P7JCsrV660BQcH27Zv325LSEiwnTx50rZhwwZb06ZNbaGhobZTp07Zjh07ZqtVq5Ztzpw5DuseOHDAds899zi09+nTxxYcHGxbvnx5pm2NGjXKVr9+fYe2vHxOtmzZ4rBsxmPt3LmzLTU11d4+cuRIW0hIiO3pp592WL5Hjx62du3aObRd/7mw2Wy2gQMH2jp06ODQllHD7t277W0JCQm2OnXq2KZMmWJve+edd2zBwcG2zZs3Z+rXarXabDabbc2aNbaaNWs69GWz2WzLly+3BQcH2/bu3ZtpXQBA1hhmDwDI1oULFyRJPj4+OVr++++/lyQNGDDAoX3gwIEO92eoUaOGGjRoYL9dr149SVeHO1eoUCFT+9GjRzNt89ojiAaDQb1791ZaWpp27Nhhb792fnFSUpKSk5PVqFGjTEP3patHomvUqHGTRyqZTCb98ssvmYbMZ/j999+VkJCgnj17Osy3b9u2rYKCgrI8z0DGSIQMjRo1chjynZXt27crLS1N/fr1cxi18MQTT8jX1zfTc54XXbp00eHDh/Xrr7/q8OHD+u2339SlS5csl92yZYsCAwMdhuB7eHiob9++unTpknbv3m1fzt3d3eExu7m5qU+fPg79nT9/Xjt37lSnTp104cIFnTt3TufOnVNiYqJatWqlf//9N9vX4Gb69++vsLAwtWnTRiNGjJCPj49mz56tsmXLKjY2VlarVZ06dbJv89y5cypdurTuuuuuTJe/8/T0VLdu3XK03dx+TipVqqR77703y766du0qDw8P++3Q0FDZbDY9/vjjDsuFhobq5MmTSk9Pt7dd+7lITk7WuXPn1LRpUx09ejTTdIcaNWo4XMowICBA1apVc/hMbt68WTVr1sxypITBYJAkbdy4UdWrV1dQUJDD85oxxcHZLisIAM6MYfYAgGz5+vpKujp8NieOHz8uo9GoKlWqOLQHBgbKZDLp+PHjDu3XX+4s48Rs5cqVy7KOa+eAS5LRaFTlypUd2jKGIF+7rW+//VZz5szRn3/+6TAvNyNgXKtSpUrZP8BrvPjiixo9erTatm2r2rVrq02bNnr00Uft9Zw4ccKhnmsFBQVp7969Dm3FihVTQECAQ5ufn5+SkpJuWEfGdoKCghzaPT09Vbly5UzPeV7cc889CgoK0vr162UymRQYGGgPX9c7fvy47rrrLocfFiSpevXqDvUeP35cgYGBmX4ouv75OnLkiGw2m9555x298847WW4zISFBZcuWzfXjGjdunKpVqyY3NzeVLl1a1apVs9f977//ymaz6YEHHshyXXd3xz+hypYtm+OT3OX2c3Kj9+S1P3pJ//sMZfXZslqtSk5Otl8eb+/evYqOjtbPP/+sy5cvOyyfnJzscKLE6/uTMr8/jxw5ku3zleHw4cM6ePCgwsLCsrw/4ySNAICbI8wDALLl6+urMmXK6O+//87VelmF5Kxkd6m37Npt153YLif27NmjZ599Vk2aNNGrr76qwMBAeXh4aOXKlVq/fn2m5XN6lvCHHnpIjRs3VmxsrLZt26ZFixZpwYIFio6OVps2bXJdp7Nf9q5z585avny5fHx81KlTp0xhvaBYrVZJV49aZ3d0+vpQnFOhoaH2s9lntV2DwaAFCxZk+dp4e3s73M7L2eVz+jm5Ud/ZvQ7ZtWd8ho4cOaL+/fsrKChIo0ePVvny5eXh4aHvv/9eS5YssT/vGfLr/Wm1WhUcHKwxY8Zkef/1P+QBALJHmAcA3FC7du308ccf66effnIYEp+VihUrymq16vDhw/YjsZJ09uxZmc3mbE8ml1dWq1VHjx51OJp76NAhey2StGnTJhUrVkyLFi1yOHK6cuXKW95+mTJl1Lt3b/Xu3VsJCQl67LHHNHfuXLVp08Z+xPTQoUOZjkIeOnQo0xHVvMroJy4uzmGUQmpqqo4dO6YWLVrky3a6dOmiWbNm6cyZM5o2bVq2y1WsWFEHDhyQ1Wp1CJRxcXEO9VasWFE7d+7UxYsXHY7OZ7x+GTIek4eHR749lpyoUqWKbDabKlWqlOXoiltR2J+TrHzzzTdKTU3VnDlzHN6LtzLMvUqVKjf94a9KlSr673//q7CwsBz/mAEAyBpz5gEAN/T000/L29tbY8eO1dmzZzPdf+TIES1dulSS7EekM25neP/99x3uz08ffvih/d82m00ffvihPDw87AHazc1NBoPB4RJvx44du6XLmlkslkxzikuVKqUyZcrYh/HXqVNHpUqV0ooVKxyG9n///fc6ePBglmeCz4sWLVrIw8NDMTExDiMXPvvsMyUnJ+fbc16lShW9/PLLeuGFFxQaGprtcq1bt9aZM2f0xRdf2NvS09MVExMjb29vNWnSxL5cenq6li9fbl/OYrFo2bJlDv2VKlVKTZs21ccff6zTp09n2t61lzzLTw888IDc3NyyvNShzWZTYmJinvsuis/J9TKOtF/72JKTk2/pR64HHnhA//3vfxUbG5vpvoztdOrUSfHx8frkk08yLZOSkqJLly7lefsAcKfhyDwA4IaqVKmi6dOna8SIEXrooYfUtWtXBQcHKzU1VT/99JM2btxoP/FXzZo19dhjj+njjz+W2WxWkyZN9Ntvv2n16tW67777sp1nnVfFihXTDz/8oFGjRik0NFQ//PCDvvvuOw0ePNg+/7xNmzZ6//339fTTT6tz585KSEjQRx99pCpVqujAgQN52u7FixfVpk0bdezYUTVr1pS3t7e2b9+u3377TaNHj5Z09Ujyiy++qDFjxqhPnz56+OGH7Zemq1ixovr3758vz0FAQIAiIyM1e/ZsPf3002rfvr0OHTqkjz76yH6N9/yScVm5G+nRo4c+/vhjjR49Wvv371fFihW1adMm7du3Ty+//LL9/Aft27dXw4YN9dZbb+n48eOqUaOGNm/enOV15l999VX16tVLXbp00ZNPPqnKlSvr7Nmz+vnnn3Xq1CmtXbs23x5jhipVqmj48OH2+u677z75+Pjo2LFj+uqrr/Tkk0/qqaeeylPfhf05yUrLli3l4eGhwYMHKzw8XBcvXtSnn36qUqVK6cyZM3nq86mnntKmTZs0bNgwPf7446pdu7aSkpL0zTffaMKECapZs6a6du2qL7/8Uq+++qp27dqlhg0bymKxKC4uThs3btTChQuznfoAAHBEmAcA3FSHDh20du1aLVq0SF9//bWWL18uT09PhYSEaPTo0XryySfty06aNEmVKlXS6tWr9dVXX6l06dKKjIx0uL51fnFzc9PChQs1fvx4TZs2TT4+PhoyZIiioqLsy4SFhen111/XggUL9MYbb6hSpUp68cUXdfz48TyHeS8vL/Xs2VPbtm3T5s2bZbPZVKVKFXvozNCtWzd5eXlpwYIFmj59ury9vXXffffp//7v/xyuUX6rnn/+eQUEBGjZsmWaPHmy/Pz89OSTT2rkyJEOZzovDF5eXoqJidH06dO1evVqXbhwQdWqVdPkyZMdzvZuNBo1Z84cvfHGG1q7dq0MBoPat2+v0aNH69FHH3Xos0aNGlq5cqVmz56t1atX6/z58woICNA999zj8Frnt2eeeUZVq1bVkiVL7Nd+L1eunFq2bKn27dvfUt+F+TnJSlBQkGbNmqW3335bb775pkqXLq2ePXsqICBAL7/8cp769PHx0Ycffqjo6GjFxsZq9erVKlWqlMLCwuwnKDQajXr33Xe1ZMkSff7554qNjVXx4sVVqVIl9e3bN9+nNADA7cxgy8vZhAAAKGKjR4/Wpk2b9NNPPxV1KQAAAIWOOfMAAAAAALgYwjwAAAAAAC6GMA8AAAAAgIthzjwAAAAAAC6GI/MAAAAAALgYwjwAAAAAAC6GMA8AAAAAgItxL+oCnJnNZpPVyikFUPCMRgPvNQC3FfZrAG437NdQGIxGgwwGQ46WJczfgNVq07lzF4u6DNzm3N2NKlnSR2bzJaWnW4u6HAC4ZezXANxu2K+hsAQE+MjNLWdh3umG2R88eFADBgxQ/fr11bJlS02dOlWpqak3XS8xMVHjxo1T27ZtVb9+fXXu3FnLly8vhIoBAAAAAChcTnVkPikpSREREapataqio6MVHx+vKVOmKCUlRePGjbvhusOGDVNcXJxGjhyp8uXLa8uWLRo/frzc3Nz05JNPFtIjAAAAAACg4DlVmF+xYoUuXryo2bNny9/fX5JksVg0YcIERUZGqmzZslmud+bMGe3atUuTJ09Wt27dJElhYWH67bfftGHDBsI8AAAAAOC24lTD7Lds2aKwsDB7kJekTp06yWq1atu2bdmul56eLkkqUaKEQ7uvr69sNk5SAQAAAAC4vThVmI+Li1NQUJBDm8lkUmBgoOLi4rJdr3z58mrVqpXmzp2rf/75RxcuXNAXX3yhbdu2qXfv3gVdNgAAAAAAhcqphtmbzWaZTKZM7X5+fkpKSrrhutHR0RoxYoQefvhhSZKbm5vGjh2rjh073lJN7u5O9XsHbkNubkaH/wOAq2O/BuB2Uxj7NavVovR0iyRGFt+eDHJ3d5PR6JZvPTpVmM8rm82mMWPG6N9//9Vbb72lwMBAbd++XW+88Yb8/PzsAT+3jEaDSpb0yedqgayZTMWLugQAyFfs1wDcbgpiv2az2XTy5EmdP39ezBC+vRkMkr+/v8qXL5/ja8nfiFOFeZPJpOTk5EztSUlJ8vPzy3a97777Ths3btTatWsVEhIiSWrWrJkSEhI0ZcqUPId5q9Ums/lSntYFcsrNzSiTqbjM5suyWLhuKQDXx34NwO2mIPdriYlndfHiBfn6+svTs1i+hDw4H5vNptTUK0pISFRKSppKliyd5XImU/EcjwBxqjAfFBSUaW58cnKyzpw5k2ku/bX++ecfubm5KTg42KG9Vq1a+vTTT3X58mUVL563X9HS0/kjBIXDYrHyfgNwW2G/BuB2k9/7NavVoosXk+XrW1K+vpmnG+P24ulZTJJ04UKifHz8ZTTe2rQNp5rM1rp1a23fvl1ms9netnHjRhmNRrVs2TLb9SpWrCiLxaIDBw44tO/fv1+lSpXKc5AHAAAAgIJisVgk/S/k4faX8VpbLOm33JdThfnw8HD5+PgoKipKW7du1cqVKzV16lSFh4c7XGM+IiJC999/v/1269atVaFCBQ0dOlSff/65duzYoWnTpmn16tXq06dPUTwUAAAAAMgRhtbfOfLztXaqYfZ+fn5aunSpJk6cqKioKPn4+Kh79+4aMWKEw3JWq9X+K5Z09XryS5Ys0cyZMzV9+nQlJyerUqVKGj16NGEeAAAAAHDbMdhsnDMxOxaLVefOXSzqMnCbc3c3qmRJHyUmXmRuKYDbAvs1ALebgtqvpaWlKiHhpEqVKi8PD0+H+4xGg4zGojlib7XaZLXmPSZGRPTUwYN/6913F6hevQb5WJnru9FrLkkBAT6ueQI8AAAAALjTGY0G+ft7F+h17W/EYrHq/PlLeQr0cXEHdfDg35Kk2NiNhPkCRJgHAAAAACdiNBrk5mbU9A/36lh85kt3F6RKZUvoxd6NZDQa8hTmY2OvnsC8fv2G+vbbrzR8+P/J3b3oY2dqaqrc3d1v+QzyzqTon1UAAAAAQCbH4pN18HhSUZeRYzabTV99tUkNGzbWE0/01KhRI7Rz53a1atXavsy//x7S/Pnv6aef9io19YoqVaqiPn0idP/9D0q6en60Tz75SOvWrdGJE8dVooRJoaH1NXr0f+Tr66vXXx+v//73D8XEfGLvMzk5WZ06tdPLL7+qhx7qIknq3r2LWrRopbJly2nVqk91+nS81q2LVVLSeS1ePE+//farkpLOq3z5Cnr44a7q0aOXQ9BPTU3VkiULFRu7SWfPnpa/f0k1btxUr7wyXlu3btHo0SO1fPkqVa5cxb6O2WzWo4920pAhw9Wt2xMF/XQT5gEAAAAAt+63337RyZMn1L//02rWLEx+fn6Kjd1oD/NHjx7R4MEDVKZMWQ0f/qICAkrp0KGDio8/Ze9j5sxpWrt2lZ58speaNGmmS5cuavv2rbp8+ZJ8fX1zVc/333+jSpWqaNiwF2U0GlW8uJf++ee0qlSpqvvv7yRvb2/9889fWrRoni5fvqSBA5+xrzt27Evau3e3+vYdoNq16+r8+UR9//23kqSwsJYKDCyjDRvWavDgIfZ1YmM3SpL9h4mCRpgHAAAAANyy2NhN8vQspjZt2svd3V1t23bQpk1f6NKlS/L29tbixfPl7u6hOXMWycfnajBv0qSZff0jRw5rzZrP9Mwzz6lv3wH29rZtO+SpnvT0dE2fPkvFixe3tzVu3FSNGzeVdHUkQWhofaWkpGjlyk/sYX737p3avn2rXn11kkMwz/i3m5ubHnqoizZsWKtBg56Vm5ubJGnDhrVq06adSpQokad6c+v2mTAAAAAAACgS6enp+vbbrxQW1sJ+BP3++x9USkqKtmy5ekR7797datu2gz3IX2/fvt2y2Wzq3LlrvtTUoEEjhyAvSVeuXNGiRfPUo8ejatcuTG3bNtf8+e8pIeGsLl26JEnas2e3vLy8dN99HbPtu3PnrkpIOKtdu3ZIkv7552/99dd/8632nODIPAAAAHCHKcrLnrmijLPK5/fZ5a3W2+c12L17p86fT1TLlq2VnHz1pH1BQTVUqlRpxcZu0oMPPqykpPMqXbp0tn0kJSXJzc1NJUsG5EtNJUuWytQ2Z0601q1brQEDBikkpJZKlCihH374XkuXLlJqaqq8vb1lNiepVKnSMhiyf33Kl6+gJk2aaf36z9WiRStt2LBW5ctXVMOGjfOl9pwgzAMAAAB3kKK+7NmN2KxWGZz0bONWq1UmU/GbL5gLKSluOnvWKDc3g9zd//e4neG1yW0NX321SZL0xhsTJE1wuO/8+UQlJp6Tn5+/zp49m20ffn5+slgsSkw8l22g9/T0VFpaukNbcrI5y2WzyuLffvuVunbtpj59+tvbtm/f6rCMyeSnhISzstlsNwz0Xbo8qgkTxurMmdOKjf1S3buH33D5/EaYBwAAAO4gRXnZsxtpWLOM+j10j06veVupCceKuhwH3kENFNCut2btXKzj5lM3XyGHShh91LZkU1kuuMng7mZv9/IsJpOpcr5tJy9S4uOVdvlKzpa9ckVbvv9OLZs01+MPd7G3G9zddcFm1X/+M0Zff71ZjRs31Xfffa3nnnte3t4+mfpp2LCJDAaDNmxY6xC2rxUYWEZnzsTb5+FL0o8/7szx47py5Yrc3T3sty0Wi77+erPDMo0bN9WHHy7VN9/EqkOHB7Lt695726pECZMmTBgrs9msTp0657iO/ECYBwAAAO5AznbZs0plrs6jTk04ptRTh4q4GkcepSpKko6bT+lQ4tF86zfAw0/ppnSlWtIlWe3tbka37FcqJNbUNFmv5CzMb92+VZdTLuux+zsqtEawvd1YrJi8K1dSTMwSxcZu0tixE7R9+w969tmn1bt3P5UqVVr//hunlJQU9e4doSpV7lLXro9rwYI5MpvNaty4qVJSUrRjx1YNHPiMAgPLqE2b9lq0aJ4mT35NjzzyqA4ditO6dWty/LiaNGmmdevWqFq1IPn5+Wv16k+VmpqWaZmwsJaaPPk1HT9+TPfcU0dms1nfffe1Xnttsn05d3d3der0sD76KEZNm4apbNlyOa4jPxDmAQAAAMAJVSpbOGdFv9VtfrNjq8qUKq16tWpnef+DD3bWrFlvyWg0as6cxZo3b7beemuKLBaLKleu4nAUfuTIl1ShQgWtXbtGn3zykfz8/FS/fkP7Ufhq1YL0yivjtWTJQo0e/YJCQ+tr3LhJGjCgV45qHTHi/zRt2mTNnDlNXl5e6tSps1q3bqc335zksNykSVP1/vsL9Pnnq7R48XwFBJRyOPN+htat2+mjj2L08MOP5PDZyj8Gm81mK/StugiLxapz5y4WdRm4zbm7G1WypI8SEy8qPd168xUAwMmxXwOcW8ZndPiM75zqyHybBhX1Yp/GOrboRac7Mu9Tu5XKPjpCoza/ke9H5sMrPSxf/wDpmjnqnu7uuqdSkNzdiuYIvSXdovj9f8ly3RHr3Mo4Mn87fxcsXDhXq1d/qtWrv5Snp+dNl09LS1VCwkmVKlVeHh6Zlw8I8Mnx+Qo4Mg8AAAAATiQ1PV1/HIuTu/Hmca24h5dKefsrJT5e1lsM3xms6em3HORvd0eO/KsjRw7rs88+VrduT+QoyOc3wjwAAAAAOJnU9HSlKv2myxlllMViVdrlKzme445bN3XqG/rjj9/VrFmY+vYdUCQ1EOYBAAAAAMiF2bPnF3UJKvoLGAIAAAAAgFwhzAMAAAAA4GII8wAAAAAAuBjCPAAAAAAALoYwDwAAAACAiyHMAwAAAADgYgjzAAAAAAC4GK4zDwAAAABOxtPdXe7Gm8e1Yh6ecnMzyqN4MVnd8udYrTU9XZbUtFyvt3TlJ4pZ/an9tl8Jk2oEB2vgwEjVq9cgX2q7kddfH6///vcPxcR8kqPlv/hind54Y4LWr/9K/v7+BVtcASDMAwAAAIAT8XR3V+1K1eTmlvO4ZjJVzbftW9PTdWr/33kK9MU8PTXt5VclSQlmsz5cu0rDhj2rxYuXKSioRr7VmJX+/Z/W5cuXc7x8WFgrzZ37vnx9fQuwqoJDmAcAAAAAJ+JudJebm7tOr3lbqQnHCnXbnqUqqcyjw2V0d89TmDcYDLqnRrAkyVismBq0bq3HHuusNWtWauTIUQ7L2mw2paWlydPTM19qr1ixUq6WL1mypEqWLJkv2y4KhHkAAAAAcEKpCceUeupQUZdxS8qVKy9//5I6efKEfRj8c88N1dy57+rw4UN69dVJatfuPv3++6+aP/89/fHH73Jzc1NYWCsNG/aCSpYMsPeVmpqqJUsWKjZ2k86ePS1//5Jq3LipXnllvKTMw+yTk5P13nvvaMeObTKbk+TvX1J164ZqwoTJkrIeZm82J2n27Le1bdsWXb6couDgEA0ePET16ze01zFkyDPy9vZWp06dNX/+ezp79oxq1aqtUaPG5voHhVtBmAcAAAAAFIiLFy/IbE5S6dKBSk9P19mzZ/X229MVEfGUypYtp7Jly+n333/V889HqnnzlpowYbJSUi5rwYI5Gj36Bc2b9769r7FjX9LevbvVt+8A1a5dV+fPJ+r777/NdtvR0TO0a9d2DR78vMqVK6+EhLPauXN7tstbLBa98MJQnTx5XM8++7xKliylzz5boREjojRnzmLVrFnLvuzff/+lxMQYDR78vKxWi6KjZ+q11/7jUG9BI8wDAAAAAPKNxWKRJJ0+Ha/5774ti8Witm076KuvNik52azp02epdu069uWnTJmomjVr6Y03pslgMEiSgoJqqF+/HtqxY6vCwlpp9+6d2r59q159dZLuv/9B+7rX/vt6f/65X/fd96A6depsb7vvvo7ZLr9jx1b9+ed+vfVWtJo1C5MkNWsWph49HlVMzGK9/vo0+7IXLiRr8eIP7cP0L1++rDfemKDTp+NVpkzZ3DxdeUaYBwAAAADki5QrV9QxItx+22QyacSIl9SsWZi++mqT/Pz8HIJ8SkqKfvvtF0VFDbP/CCBJlStXUZkyZfXnn38oLKyV9uzZLS8vrxuG8esFB9fUl1+uV6lSpdW8edhNT8D3yy8/y8fHxx7kJcnd3V1t2rRTbOwmh2Vr1Ah2mG9ftWo1SdLp06cJ8wAAAAAA11LM01Mzxr4mg0HyL1VaVevXl9X6v/tLlizlsHxyslkWi0WzZs3QrFkzMvV3+nS8pKtz2UuVKm0/cp8TI0a8JJNpnj7+eJnee+8dlSlTVn37DtBjj3XPcvnkZLPDHP1razabkxzaSpQo4XDbw8NDkpSaeiXH9d0qwjwAAAAAIF8YDAaFBFWXdPVs9kajUdZr0vz1WdzXt4QMBoP69h2g1q3bZurPz89fkmQy+Skh4axsNluOA72vr6+GDXtBw4a9oIMH/9Gnny7XW29NUVBQ9Syve28ymZSYeC5Te2JigkwmvxxtszAZi7oAAAAAAMCdqXjx4qpTp64OHz6kmjXvyfRf+fIVJEmNGzdVSkqKvvkmNk/bqV69hoYOHSlJ+vffrK8QEBpaXxcvXtSPP+60t6Wnp2vLlu8UGlovT9stSByZBwAAAAAUmeeeG6Zhw57VuHFj1KHDAypRooTOnDmt3bt36aGHuqhhw8Zq0qSZwsJaavLk13T8+DHdc08dmc1mfffd13rttclZ9vvsswN1773tFBRUXW5uRm3cuEEeHh5ZHpWXpLCwVqpVq7Zee+0/Gjx4iAICSumzzz5WQsJZ9e07sCCfgjwhzAMAAACAE/IsVXjXLC/KbdatW0/vvbdQixbN0+TJE5SWlqbAwLJq3LiJKlWqbF9u0qSpev/9Bfr881VavHi+AgJKqUmTZjfsd9OmDTpx4oSMRoOCgmrozTdn2k9Wdz03NzdNn/6O3n33Hb333iylpFxWcHBNzZgx2+GydM7CYLPZbEVdhLOyWKw6d+5iUZeB25y7u1ElS/ooMfGi0tOtN18BAJwc+zXAuWV8RofP+E4HjyfdfIVC0qZBRb3Yp7GOLXpRqaeyHgZdVHxqt1LZR0do1OY3dCjxaL71G+Dhp/BKD8vXP0By+98MaE93d9WuVE1ubkVz7NWanq5T+/+WJTXtlvoxFism78qV+C64RlpaqhISTqpUqfLy8PDMdH9AgI/c3HI2G54j8wAAAADgRFLT07X/2CG5G28e14p7eKmUt79S4uNlvcXwncGann7LQR4FjzAPAAAAAE4mNT1dqUq/6XJGGWWxWJV2+YqsVwrvsmgoepzNHgAAAAAAF0OYBwAAAADAxRDmAQAAAABwMcyZBwDgJoxGg4xGQ1GX4TIyzsKb07Px4n+sVpusVi40BAC4OcI8AAA3YDQa5O/v7ZTB1Ga1ymB0vrokyWq1ymQqXtRlZMmpnzeLRYnnLxPoAQA3RZgHAOAGjEaD3NyMmv7hXh2LTy7qcuwa1iyjfg/do9Nr3lZqwrGiLseBd1ADBbTrrVk7F+u4+VRRl+Ogfrna6hnaVX/NeFuXjjrZ81a5koJHDpfRaCDMAwBuijAPAEAOHItP1sHjSUVdhl2lMr6SpNSEY0o9daiIq3HkUaqiJOm4+ZQOJR4t4mocVShRVpJ06egxXYxzrucNAIDccLowf/DgQU2aNEk//fSTfHx81LVrVw0fPlyenp7ZrrNr1y7169cvy/uqVaumjRs3FlS5AAAAAABJS1d+opjVn9pvm3xLqGpQdfXt219hYa0KvZ59+/Zo6NDBWrjwA9WseY8kqVWrxnruuWHq1atvodeT35wqzCclJSkiIkJVq1ZVdHS04uPjNWXKFKWkpGjcuHHZrle7dm19/PHHDm0XLlzQoEGD1Lp164IuGwAAAADylae7u9yNN49rxTw85eZmlEfxYrLm0/ldrOnpsqSm5WndYp6emvbyq5KkcxeStWL95xo1aqTefXeB6tatly/14SqnCvMrVqzQxYsXNXv2bPn7+0uSLBaLJkyYoMjISJUtWzbL9Xx9fVW/fn2HtlWrVslqtapz584FXDUAAAAA5B9Pd3fdUylI7m5uOV7HZKqab9u3WNIV//vfeQr0BoNB99QIliQZixVTw9Zt1bVrJ3355XrCfD5zqjC/ZcsWhYWF2YO8JHXq1Emvvvqqtm3bpm7duuW4r/Xr16tq1aoKDQ0tgEoBAAAAoGC4G93l7uZWJCcSrWgqp6HNB8ro7p7no/PXKlOmjPz9Syo+Pt7e9vvvv2r+/Pf0xx+/y83NTWFhrTRs2AsqWTLAvkxqaqqWLFmo2NhNOnv2tPz9S6px46Z65ZXx9j5iYt7Xf//7py5evKBKlaooPLy3Hnzw4Vuu2VU4VZiPi4vT448/7tBmMpkUGBiouLi4HPdz9uxZ7dy5U88++2x+lwgAAAAAhcIZTySaW5cuXZLZnKTy5StIuhrCn38+Us2bt9SECZOVknJZCxbM0ejRL2jevPft640d+5L27t2tvn0HqHbtujp/PlHff/+t/f5Tp06qbt16evTRx+XpWUy//faLpkyZKJvNpk6d7ozR2U4V5s1ms0wmU6Z2Pz8/JSXl/AzCX3zxhSwWS74MsXd3d87r0OL2kXHtame8hjUAPpsofLznUNB4j6GgWSwWSdKZM2e0aN5seXv76MknwyVJc+fOVs2atfTGG9NkMBgkSUFBNdSvXw/t2LFVYWGttHv3Tm3fvlWvvjpJ99//oL3fa/99330d7f+22WyqV6+BTp+O1+efr3KJMO/mZrjlrOlUYT6/rFu3TrVr11a1atVuqR+j0aCSJX3yqSrgxkym4kVdAgDACfB9AMCVpVy5oo4R4fbbbm5umjp1hoKCgpSSclm//faLnn9+uAwGmySbJKlataoqW7asDhz4U/fe21r79u2Rl5eXHnywkz3wX89sNmvBgrn64YfvdObMGfsPCH5+/vaQfO1Bs2uDc34E6byyWg0yGo3y8/OWl5fXLfXlVGHeZDIpOTk5U3tSUpL8/Pxy1MeRI0f066+/asyYMbdcj9Vqk9l86Zb7AW7Ezc0ok6m4zObLslisRV0OgOtkfEaBwsL3AQoa+zUUpGKenpox9jXZbFadSDirhSs+1IQJ4/TBBx/LZrPKYrHo7bff0ttvv5Vp3VOnTik93arz58+rVKnSslj+F/iv99pr4/T777+qf/+nVa1adfn4+Gj16s/0zTexSk+/ug/N2JdaLFZ729XbNofbhclisclqtSop6ZIuX7Zkut9kKp7j0TNOFeaDgoIyzY1PTk7WmTNnFBQUlKM+1q1bJ6PRqIceeihfaiqqFxl3nut3MgCAOxPfBwBcmcFgUEhQdUnSPffU0d0NGuqpp/ppyZIFiooaLoPBoL59B6h167aZ1vXz85ckmUx+Skg4K5vNluWR+StXrmj79q0aMmSEunf/3ygAmy3r4O+M8uMHBaeaMNO6dWtt375dZrPZ3rZx40YZjUa1bNkyR31s2LBBTZs2VZkyZQqqTAAAAABADtSqdY/uu6+jvvhinS5duqg6derq8OFDqlnznkz/ZZwkr3HjpkpJSdE338Rm2WdaWpqsVqs8PDzsbZcuXdTWrVsK5TE5C6c6Mh8eHq6YmBhFRUUpMjJS8fHxmjp1qsLDwx2uMR8REaETJ04oNtbxxf3jjz908OBBDRgwoLBLBwAAAABkoX//p/T115v1ySfL9dxzwzRs2LMaN26MOnR4QCVKlNCZM6e1e/cuPfRQFzVs2FhNmjRTWFhLTZ78mo4fP6Z77qkjs9ms7777Wq+9Nlm+vr6qVeseLVu2RP7+/nJzc9eyZUvk4+Or8+fPFfXDLTROFeb9/Py0dOlSTZw4UVFRUfLx8VH37t01YsQIh+WsVqv9BAfXWrdunTw9PdWxY8dM9wEAAACAK6loKndbbLNKlarq0OEBrVnzmfr2HaD33luoRYvmafLkCUpLS1NgYFk1btxElSpVtq8zadJUvf/+An3++SotXjxfAQGl1KRJM/v9r776uqZNe0Ovvz5eJpOfuncP1+XLl7RixbJ8r99ZOVWYl6Tq1atryZIlN1wmJiYmy/ZRo0Zp1KhRBVAVAAAAABSOdGu60i0WDW0+sEi2b7Gky5qenuv1Ih5/UhGPP5nlfePGTbT/u2bNezRt2js37KtYsWIaPHiIBg8ekuX9lSpV1jvvzMnU/tRTkfZ/N2zYWFu37nG4//rbrszpwjwAAAAA3MlS09P1x7E4uRtvHteKe3iplLe/UuLjZU1Ny5ftW9PTZcmnvlBwCPMAAAAA4GRS09OVqpsfHTfKKIvFqrTLV2S9cqUQKoOzcKqz2QMAAAAAgJsjzAMAAAAA4GII8wAAAAAAuBjCPAAAAAAALoYwDwAAAACAiyHMAwAAAADgYgjzAAAAAAC4GMI8AAAAAAAuxr2oCwAAAAAAOPJ0d5e78eZxrZiHp9zcjPIoXkxWt/w5VmtNT5clNS3X6y1d+YliVn+aqb1atSDFxHyi3bt3asOGdfrjj9914sRxdev2hEaOHJWjvtPT07VmzWdat+5znThxTG5u7ipbtpzq1auvIUNGyNPTM9f1ujrCPAAAAAA4EU93d9WuWE1u7jmPayZT1XzbvjU9Xaf2/52nQF/M01PTXn5VkmT08JBX2TJydy8mSdq5c4f++edv1a/fUGazOVf9vv32dH3xxTr17dtfdeqE6sqVFP3991/atOkLXblyhTAPAAAAACha7kZ3ubm7668Zb+vS0WOFum3vypUUPHK4jO7ueQrzBoNB99QIliQZixWTd+VKSk+3SpKioobp+edHSJL27duT4z5TUlK0fv0aRUQ8pQEDBtnbW7VqowEDBslms+W6zty6ciVFxYp5Ffh2coMwDwAAAABO6NLRY7oYd6ioy8g3RmPepgFcvnxZ6enpKlWqdJb3GwwG+7+tVqs++eQjrVu3RidOHFeJEiaFhtbX6NH/ka+vryTp55/3ae7c2frrrwMqXtxLLVu21pAhw2Uy+UmSTp48oSeeeEQvv/yqfvvtF33//bcqXbq0PvjgY6Wmpur99xdo8+Yvde5cgipUqKiIiKf1wAMP5umx3QrCPAAAAAAg31gsFkmSzWJRenq6bDaDQ+DOrZIlS6ps2XJaunSRvL291bRpmEwmU5bLzpw5TWvXrtKTT/ZSkybNdOnSRW3fvlWXL1+Sr6+v/vvfPzViRJQaNGikiROnKDExQXPnztahQ3GaO3ex3Nzc7H3NmzdbYWGtNH7867Jar44uGDdutH799RcNGDBIVatW1Y4d2zRx4n9UokQJhYW1zPNjzAvCPAAAAAAgX6RcuaKOEeEObf/5z2vq2PGhW+r3lVfGa/z4VzR+/CsyGAy6666qatWqjcLD+8jf31+SdOTIYa1Z85meeeY59e07wL5u27Yd7P/+4IPFCggopalT35b7/z8nQZky5TRy5BDt2LFNrVq1ti9bo0aIRo/+j/32vn17tHXrFs2YMVtNmzaXJDVp0lwJCWe1ePE8wjwAAAAAwDUV8/TUjLGvSZKMnh7yKltWZcuWz/H6FovFYQ58RuBu2LCxPv54jXbu3Ka9e3dr797dWrZsib74Yp0WL16m0qUDtW/fbtlsNnXu3DXb/n/99Sfdd19He7+S1LRpc/n6ltCvv/7sEOZbtHAM5z/+uFMmk58aNmys9PR0e3uTJs00bdpkWSwWhyP7BY0wDwAAAADIFwaDQSFB1SVlPgFeTvTo8ahOnTppv/3pp2tVvnwFSVLx4sXVrt19atfuPknSunVr9Oabk7R8eYyef36kkpKS5ObmppIlA7LtPzk5Ocv7AwIClJzseIb9kiVLOdxOSjovszlJbds2z7LvhISzKlOmbM4eaD4gzAMAAAAAnMKbb85UWlqq/Xbp0oHZLtuly6OaMyda//77ryTJz89PFotFiYnnsg30JUqYlJiYmKn93LlzKlHCcR7+9dP8S5Qwyd+/pKZPfyfLvm/0I0JBIMwDAAAAAJxC9eo1MrWlp6fr0qVLmU56l5h4ThcvXlCpUlePoDds2EQGg0EbNqxVnz79s+w/NLS+fvjhOw0ZMtw+1H737p26cCFZoaH1b1hbkyZN9dFHH8jd3UM1atyd24eW7wjzAAAAAIACd+rUSf35535JV68df/z4cX377VeSZB86n5ULFy4oPPwxPfjgw2rYsLFMJpNOnjyh5ctjZDQa9eijj0uSqlS5S127Pq4FC+bIbDarceOmSklJ0Y4dWzVw4DMKDCyjfv0G6tlnB+qll0aoe/ceOnfu6tnsa9WqfdMT2DVp0lwtW96rF154Xr1791P16nfr8uXLOnQoTsePH3U4WV5hIMwDAAAAgBPyrlzpttrmvn179MYbE+y3d+3arl27tkuStm7dk+16Pj4+6tMnQrt27dC338ba573XqnWPXnllgkJCatqXHTnyJVWoUEFr167RJ598JD8/P9Wv31De3t6SpJo1a2nGjNmaN+9djR37kry8iqtVq6vXmc/JyesmTZqqZcuWaNWqzxQff1I+Pr4KCqquhx7qktenJc8I8wAAAADgRNKt6bKkpyt45PAi2b41PV3Wa87WnlMRjz+piMefzPb+hx7qkqfQ6+HhoT59+mc7dP5aRqNRvXr1U69e/bJdpkGDRpo7d3G295cvXyHbHxc8PDw0YMAgDRgw6Ka1FDTCPAAAAAA4kdT0dO0/fkjuxpvHteIeXirl7a+U+HhZU9PyZfvW9HRZ8qkvFBzCPAAAAAA4mdT0dKXq5kfHjTLKYrEq7fIVWa9cKYTK4CyMRV0AAAAAAADIHcI8AAAAAAAuhjAPAAAAAICLIcwDAAAAQBGwyVbUJaCQ2Wz595oT5gEAAACgCFxMvyyLzSJZrEVdCgpJaurVkxS6ud36ueg5mz0AAAAAFIFUW5p+TTqgxm51VVwlJLfcH2u1pqcrNfWK0m1WWZ3sSL/RZlVq6hVZLM5VV1Gw2WxKTb2iCxcSVby4r4zGWz+uTpgHAAAAgCKyI/FnSVKoJURuBrdcr5/mXky2S1eUmpwoW/rNL2VXmAxX3OXpbpPVysiDDMWL+8pkCsiXvgjzAAAAAFCEdiT+rL3n98vHvbgMMuRq3Ybl66jfPd315+Q3lXL0eAFVmDfFK1dU9TGjlJR0iaPzujq0Pj+OyGcgzAMAAABAEUu1pSk1LS3X613WFXl5eclgTpYtIaEAKss7g59JXl5eunzZovR0js7nN06ABwAAAACAiyHMAwAAAADgYgjzAAAAAAC4GMI8AAAAAAAuhjAPAAAAAICLIcwDAAAAAOBiCPMAAAAAALgYwjwAAAAAAC6GMA8AAAAAgItxujB/8OBBDRgwQPXr11fLli01depUpaam5mjd+Ph4jRo1Ss2bN1doaKg6deqktWvXFnDFAAAAAAAULveiLuBaSUlJioiIUNWqVRUdHa34+HhNmTJFKSkpGjdu3A3XPX36tHr06KFq1app4sSJ8vX11d9//53jHwIAAAAAAHAVThXmV6xYoYsXL2r27Nny9/eXJFksFk2YMEGRkZEqW7ZstutOmzZN5cqV08KFC+Xm5iZJCgsLK4yyAQAAAAAoVE41zH7Lli0KCwuzB3lJ6tSpk6xWq7Zt25btehcuXNCXX36pXr162YM8AAAAAAC3K6cK83FxcQoKCnJoM5lMCgwMVFxcXLbr7d+/X2lpaXJ3d1efPn1Uu3ZttWzZUtOmTVNaWlpBlw0AAAAAQKFyqmH2ZrNZJpMpU7ufn5+SkpKyXe/s2bOSpLFjx+rJJ5/UkCFD9Ouvv2rWrFkyGo164YUX8lyTu7tT/d6B25Cbm9Hh/wCcC59NFDbecyhovMdQ2HjPFQynCvN5ZbVaJUktWrTQ6NGjJUnNmzfXxYsXtXjxYkVFRcnLyyvX/RqNBpUs6ZOvtQLZMZmKF3UJAAAnwPcBgNsN+7WC4VRh3mQyKTk5OVN7UlKS/Pz8briedDXAXyssLExz587V4cOHFRISkut6rFabzOZLuV4PyA03N6NMpuIymy/LYrEWdTkArpPxGQUKC98HKGjs11DY2K/lnMlUPMcjGZwqzAcFBWWaG5+cnKwzZ85kmkt/rRo1atyw3ytXruS5pvR03nQoHBaLlfcbAIDvAwC3HfZrBcOpJi+0bt1a27dvl9lstrdt3LhRRqNRLVu2zHa9ihUrKjg4WNu3b3do3759u7y8vG4a9gEAAAAAcCVOFebDw8Pl4+OjqKgobd26VStXrtTUqVMVHh7ucI35iIgI3X///Q7rjhgxQt98841ef/11bdu2TXPnztXixYvVv39/eXt7F/ZDAQAAAACgwDjVMHs/Pz8tXbpUEydOVFRUlHx8fNS9e3eNGDHCYTmr1SqLxeLQ1r59e82YMUPvvfeeli9frjJlyuj555/XM888U5gPAQAAAACAAudUYV6SqlevriVLltxwmZiYmCzbH3roIT300EMFUBUAAAAAAM7DqYbZAwAAAACAmyPMAwAAAADgYgjzAAAAAAC4GMI8AAAAAAAuhjAPAAAAAICLIcwDAAAAAOBiCPMAAAAAALgYwjwAAAAAAC6GMA8AAAAAgIshzAMAAAAA4GII8wAAAAAAuBjCPAAAAAAALoYwDwAAAACAiyHMAwAAAADgYgjzAAAAAAC4GMI8AAAAAAAuhjAPAAAAAICLIcwDAAAAAOBiCPMAAAAAALgYwjwAAAAAAC6GMA8AAAAAgIshzAMAAAAA4GII8wAAAAAAuBjCPAAAAAAALoYwDwAAAACAiyHMAwAAAADgYgjzAAAAAAC4GMI8AAAAAAAuhjAPAAAAAICLIcwDAAAAAOBiCPMAAAAAALgYwjwAAAAAAC6GMA8AAAAAgIshzAMAAAAA4GII8wAAAAAAuBjCPAAAAAAALoYwDwAAAACAiyHMAwAAAADgYgjzAAAAAAC4GMI8AAAAAAAuhjAPAAAAAICLIcwDAAAAAOBiCPMAAAAAALgYwjwAAAAAAC7GvagLuN7Bgwc1adIk/fTTT/Lx8VHXrl01fPhweXp63nC99u3b6/jx45naf/31VxUrVqygygUAAAAAoNA5VZhPSkpSRESEqlatqujoaMXHx2vKlClKSUnRuHHjbrp+x44dNXDgQIe2m/0IAAAAAACAq3GqML9ixQpdvHhRs2fPlr+/vyTJYrFowoQJioyMVNmyZW+4funSpVW/fv2CLxQAAAAAgCLkVHPmt2zZorCwMHuQl6ROnTrJarVq27ZtRVcYAAAAAABOxKnCfFxcnIKCghzaTCaTAgMDFRcXd9P1161bpzp16qhBgwYaNGiQDhw4UFClAgAAAABQZJxqmL3ZbJbJZMrU7ufnp6SkpBuu2759e4WGhqpChQo6evSo5s6dq169emnNmjWqXLlynmtyd3eq3ztwG3JzMzr8H4Bz4bOJwsZ7DgWN9xgKG++5guFUYf5WjB071v7vxo0bq2XLlurUqZMWLVqk8ePH56lPo9GgkiV98qlC4MZMpuJFXQIAwAnwfQDgdsN+rWA4VZg3mUxKTk7O1J6UlCQ/P79c9VWmTBk1atRI+/fvz3M9VqtNZvOlPK8P5ISbm1EmU3GZzZdlsViLuhwA18n4jAKFhe8DFDT2ayhs7NdyzmQqnuORDE4V5oOCgjLNjU9OTtaZM2cyzaUvLOnpvOlQOCwWK+83AADfBwBuO+zXCoZTTV5o3bq1tm/fLrPZbG/buHGjjEajWrZsmau+4uPjtXfvXtWtWze/ywQAAAAAoEg51ZH58PBwxcTEKCoqSpGRkYqPj9fUqVMVHh7ucI35iIgInThxQrGxsZKk9evX69tvv1WbNm1UpkwZHT16VPPnz5ebm5sGDBhQVA8HAAAAAIAC4VRh3s/PT0uXLtXEiRMVFRUlHx8fde/eXSNGjHBYzmq1ymKx2G9XqlRJp0+f1htvvKHk5GSVKFFCzZs319ChQ2/pTPYAAAAAADgjpwrzklS9enUtWbLkhsvExMQ43K5fv36mNgAAAAAAbldONWceAAAAAADcHGEeAAAAAAAXQ5gHAAAAAMDFEOYBAAAAAHAxhHkAAAAAAFwMYR4AAAAAABdDmAcAAAAAwMUQ5gEAAAAAcDGEeQAAAAAAXAxhHgAAAAAAF0OYBwAAAADAxRDmAQAAAABwMYR5AAAAAABcDGEeAAAAAAAXQ5gHAAAAAMDFEOYBAAAAAHAxhHkAAAAAAFwMYR4AAAAAABdDmAcAAAAAwMUQ5gEAAAAAcDGEeQAAAAAAXAxhHgAAAAAAF+Oen53ZbDbt3LlTqampatSokXx9ffOzewAAAAAAoFsI8zNnztS+ffsUExMj6WqQHzhwoHbu3CmbzaYKFSpoyZIlqlKlSr4VCwAAAAAAbmGY/aZNmxQaGmq/vXHjRu3YsUPDhw/XvHnzZLFYFB0dnS9FAgAAAACA/8nzkfn4+Hjddddd9tuxsbGqUaOGIiMjJUk9e/bU8uXLb71CAAAAAADgIM9H5t3d3ZWamirp6hD7HTt26N5777XfX6pUKSUmJt56hQAAAAAAwEGew/zdd9+ttWvXKikpSStXrtT58+fVpk0b+/0nTpxQyZIl86VIAAAAAADwP3keZh8VFaXBgwerefPmkqSGDRva/y1J33//verWrXvrFQIAAAAAAAd5DvMtW7bU6tWrtW3bNplMJj300EP2+5KSktS4cWN16NAhX4oEAAAAAAD/c0vXma9Ro4Zq1KiRqd3Pz08vv/zyrXQNAAAAAACycUthXpJ+/vln7dq1SwkJCerVq5eqVq2qy5cvKy4uTlWrVpWPj09+1AkAAAAAAP6/PIf51NRUjRw5Ul9//bVsNpsMBoPatWunqlWrymg0auDAgerfv7+effbZ/KwXAAAAAIA7Xp7PZv/OO+/ou+++0/jx47Vx40bZbDb7fcWKFdODDz6or7/+Ol+KBAAAAAAA/5PnML9hwwaFh4erR48e8vPzy3R/9erVdfTo0VsqDgAAAAAAZJbnMJ+QkKCQkJBs73dzc1NKSkpeuwcAAAAAANnIc5gvX7684uLisr1/3759qlKlSl67BwAAAAAA2chzmO/cubNWrFihn376yd5mMBgkSZ988om+/PJLPfroo7dcIAAAAAAAcJTns9kPHjxYv/zyi/r06aOgoCAZDAZNnjxZSUlJOnXqlNq0aaP+/fvnY6kAAAAAAEC6hTDv6emphQsXau3atdq0aZOsVqtSU1MVEhKi4cOHq2vXrvYj9QAAAAAAIP/kKcynpKRo5syZatasmbp27aquXbvmd10AAAAAACAbeZoz7+XlpY8//lgJCQn5XQ8AAAAAALiJPJ8Ar3bt2vrrr7/ysxYAAAAAAJADeQ7zL7/8sr744gt9+umnSk9Pz8+aAAAAAADADeT5BHijR4+WwWDQuHHjNGnSJJUtW1bFihVzWMZgMGjt2rW56vfgwYOaNGmSfvrpJ/n4+Khr164aPny4PD09c9zHkiVLNHnyZLVt21bz5s3L1fYBAAAAAHB2eQ7z/v7+8vf3V7Vq1fKtmKSkJEVERKhq1aqKjo5WfHy8pkyZopSUFI0bNy5HfZw5c0bvvvuuSpUqlW91AQAAAADgTPIc5mNiYvKzDknSihUrdPHiRc2ePVv+/v6SJIvFogkTJigyMlJly5a9aR/Tpk1T+/btdeLEiXyvDwAAAAAAZ5DnOfMFYcuWLQoLC7MHeUnq1KmTrFartm3bdtP19+zZo6+++kovvPBCAVYJAAAAAEDRyvOReenqUfO1a9fqu+++sx8Jr1Chgtq1a6cuXbrIzc0tV/3FxcXp8ccfd2gzmUwKDAxUXFzcTWuZOHGiBg8erDJlyuTugQAAAAAA4ELyHOaTk5P11FNP6bfffpOPj48qV64sSdq+fbs2b96s5cuXa9GiRfL19c1xn2azWSaTKVO7n5+fkpKSbrjuRx99pMuXL6t///65ehw34+7uVIMXcBtyczM6/B+Ac+GzicLGew4FjfcYChvvuYKR5zA/c+ZM7d+/X2PHjtWTTz4pDw8PSVJaWpo+/fRTvf7665o5c6b+85//5Fux2UlISNCsWbP05ptv5uqs9zdjNBpUsqRPvvUH3IjJVLyoSwAAOAG+DwDcbtivFYw8h/nY2Fj17NlTvXv3dmj38PBQr169FBcXp40bN+YqzJtMJiUnJ2dqT0pKkp+fX7brvfPOOwoJCVHjxo1lNpslSenp6UpPT5fZbJa3t7fc3XP/UK1Wm8zmS7leD8gNNzejTKbiMpsvy2KxFnU5AK6T8RkFCgvfByho7NdQ2Niv5ZzJVDzHIxnyHObPnz9/w8vSVatW7aZD468XFBSUaW58cnKyzpw5o6CgoGzXO3TokHbv3q0mTZpkuq9JkyZasGCBWrdunataMqSn86ZD4bBYrLzfAAB8HwC47bBfKxh5DvN33XWXvvnmm0xH5jN88803qlKlSq76bN26tebOneswd37jxo0yGo1q2bJltuu9/PLL9iPyGd544w15eXlp5MiRCgkJyVUdAAAAAAA4szyH+Z49e2rixIkaNGiQIiIiVLVqVUlXj5LHxMRo+/btuZ4vHx4erpiYGEVFRSkyMlLx8fGaOnWqwsPDHa4xHxERoRMnTig2NlaSVKtWrUx9mUwmeXt7q1mzZnl9iAAAAAAAOKU8h/nevXvr3Llzmj9/vrZu3erYqbu7oqKi1KtXr1z16efnp6VLl2rixImKioqSj4+PunfvrhEjRjgsZ7VaZbFY8lo6AAAAAAAu7ZauM//888+rd+/e2rFjh44fPy5JqlixosLCwhQQEJCnPqtXr64lS5bccJmYmJib9pOTZQAAAAAAcEW3FOYlKSAgQA8//HB+1AIAAAAAAHIgZ+e8z8L27ds1Y8aMbO+fOXOmduzYkdfuAQAAAABANvIc5t977z2dPHky2/vj4+M1Z86cvHYPAAAAAACykecw/9dff6levXrZ3l+3bl0dOHAgr90DAAAAAIBs5DnMp6amKi0t7Yb3p6Sk5LV7AAAAAACQjTyH+bvvvtt+nffr2Ww2bd68WdWrV89zYQAAAAAAIGt5DvN9+vTRvn37NHToUB04cEDp6elKT0/Xf//7Xw0bNkw///yz+vbtm5+1AgAAAAAA3cKl6bp27aqjR4/qvffeU2xsrIzGq78LWK1WGQwGPfvss3rsscfyrVAAAAAAAHDVLV1nfsiQIXrkkUcUGxuro0ePSpKqVKmi++67T1WqVMmXAgEAAAAAgKM8D7PPUKVKFT311FPq27evAgMDdeTIEX333Xe6cOFCftQHAAAAAACuk6sj88uWLVNMTIyWL1+ugIAAe/u3336roUOHKj09XTabTZIUExOjjz/+2GE5AAAAAABw63J1ZP6bb75R5cqVHQJ6enq6XnnlFbm5uemNN97QunXr9MILL+jEiROaO3duvhcMAAAAAMCdLldh/p9//lH9+vUd2nbt2qVz584pIiJCjz32mO6++24NGjRIDz74oL7//vv8rBUAAAAAACiXYf78+fMqV66cQ9uOHTtkMBh0//33O7Q3bNhQJ0+evPUKAQAAAACAg1yF+dKlS+vs2bMObXv27JGXl5dq1qzp0O7p6SkPD49brxAAAAAAADjIVZivU6eOVq9ebT9T/d9//63ffvtN9957r9zdHc+lFxcXl+koPgAAAAAAuHW5Opt9VFSUunfvro4dO6pGjRrav3+/DAaDnnnmmUzLxsbGqnnz5vlWKAAAAAAAuCpXR+ZDQkK0dOlS1a5dW6dPn1a9evU0f/581alTx2G5Xbt2qXjx4nrwwQfztVgAAAAAAJDLI/PS1RPbzZ8//4bLNGvWTOvWrctzUQAAAAAAIHu5OjIPAAAAAACKHmEeAAAAAAAXQ5gHAAAAAMDFEOYBAAAAAHAxhHkAAAAAAFwMYR4AAAAAABdDmAcAAAAAwMUQ5gEAAAAAcDGEeQAAAAAAXAxhHgAAAAAAF0OYBwAAAADAxRDmAQAAAABwMYR5AAAAAABcDGEeAAAAAAAXQ5gHAAAAAMDFEOYBAAAAAHAxhHkAAAAAAFwMYR4AAAAAABdDmAcAAAAAwMUQ5gEAAAAAcDGEeQAAAAAAXAxhHgAAAAAAF0OYBwAAAADAxRDmAQAAAABwMYR5AAAAAABcjNOF+YMHD2rAgAGqX7++WrZsqalTpyo1NfWm67344ot64IEHVL9+fTVp0kS9e/fW1q1bC6FiAAAAAAAKl3tRF3CtpKQkRUREqGrVqoqOjlZ8fLymTJmilJQUjRs37obrpqWlqX///qpataquXLmizz77TM8884w++OADNW7cuJAeAQAAAAAABc+pwvyKFSt08eJFzZ49W/7+/pIki8WiCRMmKDIyUmXLls123XfeecfhduvWrdWhQwd9/vnnhHkAAAAAwG3FqYbZb9myRWFhYfYgL0mdOnWS1WrVtm3bctWXm5ubSpQoobS0tHyuEgAAAACAouVUYT4uLk5BQUEObSaTSYGBgYqLi7vp+jabTenp6UpMTNSiRYt0+PBh9ejRo6DKBQAAAACgSDjVMHuz2SyTyZSp3c/PT0lJSTdd/7PPPtPYsWMlSd7e3po5c6YaNGhwSzW5uzvV7x24Dbm5GR3+D8C58NlEYeM9h4LGewyFjfdcwXCqMH+rOnTooJo1ayoxMVEbN27U8OHDNXv2bLVp0yZP/RmNBpUs6ZPPVQJZM5mKF3UJAAAnwPcBgNsN+7WC4VRh3mQyKTk5OVN7UlKS/Pz8brp+QECAAgICJF09AV5SUpKmTZuW5zBvtdpkNl/K07pATrm5GWUyFZfZfFkWi7WoywFwnYzPKFBY+D5AQWO/hsLGfi3nTKbiOR7J4FRhPigoKNPc+OTkZJ05cybTXPqcqF27trZs2XJLNaWn86ZD4bBYrLzfAAB8HwC47bBfKxhONXmhdevW2r59u8xms71t48aNMhqNatmyZa7727t3rypXrpyfJQIAAAAAUOSc6sh8eHi4YmJiFBUVpcjISMXHx2vq1KkKDw93uMZ8RESETpw4odjYWEnSd999pzVr1qht27YqX768kpKStH79em3dulUzZswoqocDAAAAAECBcKow7+fnp6VLl2rixImKioqSj4+PunfvrhEjRjgsZ7VaZbFY7LcrV66s1NRUvfXWW0pMTFTJkiUVEhKimJgYNW3atLAfBgAAAAAABcqpwrwkVa9eXUuWLLnhMjExMZnWee+99wqwKgAAAAAAnIdTzZkHAAAAAAA3R5gHAAAAAMDFEOYBAAAAAHAxhHkAAAAAAFwMYR4AAAAAABdDmAcAAAAAwMUQ5gEAAAAAcDGEeQAAAAAAXAxhHgAAAAAAF0OYBwAAAADAxRDmAQAAAABwMYR5AAAAAABcDGEeAAAAAAAXQ5gHAAAAAMDFEOYBAAAAAHAxhHkAAAAAAFwMYR4AAAAAABdDmAcAAAAAwMUQ5gEAAAAAcDGEeQAAAAAAXAxhHgAAAAAAF0OYBwAAAADAxRDmAQAAAABwMYR5AAAAAABcDGEeAAAAAAAXQ5gHAAAAAMDFEOYBAAAAAHAxhHkAAAAAAFwMYR4AAAAAABdDmAcAAAAAwMUQ5gEAAAAAcDGEeQAAAAAAXAxhHgAAAAAAF0OYBwAAAADAxRDmAQAAAABwMYR5AAAAAABcDGEeAAAAAAAXQ5gHAAAAAMDFEOYBAAAAAHAxhHkAAAAAAFwMYR4AAAAAABdDmAcAAAAAwMUQ5gEAAAAAcDGEeQAAAAAAXIx7URdwvYMHD2rSpEn66aef5OPjo65du2r48OHy9PTMdp3Tp09ryZIl2rZtm44cOaISJUqoSZMmGjlypCpWrFiI1QMAAAAAUPCcKswnJSUpIiJCVatWVXR0tOLj4zVlyhSlpKRo3Lhx2a63f/9+xcbG6vHHH1e9evWUmJioOXPm6IknntD69esVEBBQiI8CAAAAAICC5VRhfsWKFbp48aJmz54tf39/SZLFYtGECRMUGRmpsmXLZrleo0aN9OWXX8rd/X8Pp2HDhmrbtq3WrFmjgQMHFkb5AAAAAAAUCqeaM79lyxaFhYXZg7wkderUSVarVdu2bct2PZPJ5BDkJalcuXIKCAjQ6dOnC6pcAAAAAACKhFOF+bi4OAUFBTm0mUwmBQYGKi4uLld9HTp0SAkJCapevXp+lggAAAAAQJFzqmH2ZrNZJpMpU7ufn5+SkpJy3I/NZtOkSZNUpkwZPfzww7dUk7u7U/3egduQm5vR4f8AnAufTRQ23nMoaLzHUNh4zxUMpwrz+SU6Olo7d+7UwoUL5e3tned+jEaDSpb0ycfKgOyZTMWLugQAgBPg+wDA7Yb9WsFwqjBvMpmUnJycqT0pKUl+fn456uOTTz7Ru+++q9dff11hYWG3VI/VapPZfOmW+gBuxs3NKJOpuMzmy7JYrEVdDoDrZHxGgcLC9wEKGvs1FDb2azlnMhXP8UgGpwrzQUFBmebGJycn68yZM5nm0mclNjZW48eP19ChQ9W9e/d8qSk9nTcdCofFYuX9BgDg+wDAbYf9WsFwqskLrVu31vbt22U2m+1tGzdulNFoVMuWLW+47q5duzRy5Eg98cQTioqKKuhSAQAAAAAoMk4V5sPDw+Xj46OoqCht3bpVK1eu1NSpUxUeHu5wjfmIiAjdf//99tsHDx5UVFSUqlatqq5du+rnn3+2/3fkyJGieCgAAAAAABQYpxpm7+fnp6VLl2rixImKioqSj4+PunfvrhEjRjgsZ7VaZbFY7Ld/+eUXJScnKzk5WT179nRY9rHHHtOUKVMKpX4AAAAAAAqDU4V5SapevbqWLFlyw2ViYmIcbnfr1k3dunUrwKoAAAAAAHAeTjXMHgAAAAAA3BxhHgAAAAAAF0OYBwAAAADAxRDmAQAAAABwMYR5AAAAAABcDGEeAAAAAAAXQ5gHAAAAAMDFEOYBAAAAAHAxhHkAAAAAAFwMYR4AAAAAABdDmAcAAAAAwMUQ5gEAAAAAcDGEeQAAAAAAXAxhHgAAAAAAF0OYBwAAAADAxRDmAQAAAABwMYR5AAAAAABcDGEeAAAAAAAXQ5gHAAAAAMDFEOYBAAAAAHAxhHkAAAAAAFwMYR4AAAAAABdDmAcAAAAAwMUQ5gEAAAAAcDGEeQAAAAAAXAxhHgAAAAAAF0OYBwAAAADAxRDmAQAAAABwMYR5AAAAAABcDGEeAAAAAAAXQ5gHAAAAAMDFEOYBAAAAAHAxhHkAAAAAAFwMYR4AAAAAABdDmAcAAAAAwMUQ5gEAAAAAcDGEeQAAAAAAXAxhHgAAAAAAF0OYBwAAAADAxRDmAQAAAABwMYR5AAAAAABcDGEeAAAAAAAXQ5gHAAAAAMDFEOYBAAAAAHAxThfmDx48qAEDBqh+/fpq2bKlpk6dqtTU1Juu9+GHHyoyMlLNmzdXSEiINm7cWAjVAgAAAABQ+JwqzCclJSkiIkJpaWmKjo7WiBEj9Mknn2jKlCk3Xffzzz9XYmKi2rRpUwiVAgAAAABQdNyLuoBrrVixQhcvXtTs2bPl7+8vSbJYLJowYYIiIyNVtmzZG65rNBp17NgxrVmzpnAKBgAAAACgCDjVkfktW7YoLCzMHuQlqVOnTrJardq2bdsN1zUaneqhAAAAAABQYJzqyHxcXJwef/xxhzaTyaTAwEDFxcUVSU3u7vxIgILl5mZ0+D8A58JnE4WN9xwKGu8xFDbecwXDqcK82WyWyWTK1O7n56ekpKRCr8doNKhkSZ9C3y7uTCZT8aIuAQDgBPg+AHC7Yb9WMJwqzDsbq9Ums/lSUZeB25ybm1EmU3GZzZdlsViLuhwA18n4jAKFhe8DFDT2ayhs7NdyzmQqnuORDE4V5k0mk5KTkzO1JyUlyc/PrwgqktLTedOhcFgsVt5vAAC+DwDcdtivFQynmrwQFBSUaW58cnKyzpw5o6CgoCKqCgAAAAAA5+JUYb5169bavn27zGazvW3jxo0yGo1q2bJlEVYGAAAAAIDzcKph9uHh4YqJiVFUVJQiIyMVHx+vqVOnKjw83OEa8xERETpx4oRiY2Ptbb/99puOHz+uc+fOSZJ++eUXSVJAQICaNm1auA/kDmc0GmQ0Goq6DJfB2ezzzmq1yWq1FXUZAAAAQKFzqjDv5+enpUuXauLEiYqKipKPj4+6d++uESNGOCxntVplsVgc2j788EOtXr3afnvx4sWSpKZNmyomJqbgi4ekq0He39/bKYOpzWqVweh8dUlX39POeiIap37eLBYlnr9MoAcAAMAdx6nCvCRVr15dS5YsueEyWYXzKVOmaMqUKQVUFXLKaDTIzc2o6R/u1bH4zCczLCoNa5ZRv4fu0ek1bys14VhRl+PAO6iBAtr11qydi3XcfKqoy3FQv1xt9Qztqr9mvK1LR53seatcScEjh8toNBDmAQAAcMdxujCP28Ox+GQdPJ5U1GXYVSrjK0lKTTim1FOHirgaRx6lKkqSjptP6VDi0SKuxlGFElent1w6ekwX45zreQMAAADuZM45dhYAAAAAAGSLMA8AAAAAgIshzAMAAAAA4GII8wAAAAAAuBjCPAAAAAAALoYwDwAAAACAiyHMAwAAAADgYgjzAAAAAAC4GMI8AAAAAAAuhjAPAAAAAICLIcwDAAAAAOBiCPMAAAAAALgYwjwAAAAAAC6GMA8AAAAAgIshzAMAAAAA4GII8wAAAAAAuBjCPAAAAAAALoYwDwAAAACAiyHMAwAAAADgYgjzAAAAAAC4GMI8AAAAAAAuhjAPAAAAAICLIcwDAAAAAOBiCPMAAAAAALgYwjwAAAAAAC6GMA8AAAAAgIshzAMAAAAA4GII8wAAAAAAuBjCPAAAAAAALoYwDwAAAACAiyHMAwAAAADgYgjzAAAAAAC4GMI8AAAAAAAuhjAPAAAAAICLIcwDAAAAAOBiCPMAAAAAALgYwjwAAAAAAC6GMA8AAAAAgIshzAMAAAAA4GII8wAAAAAAuBjCPAAAAAAALoYwDwAAAACAi3G6MH/w4EENGDBA9evXV8uWLTV16lSlpqbedD2bzab58+erbdu2Cg0NVY8ePfTzzz8XfMEAAAAAABQypwrzSUlJioiIUFpamqKjozVixAh98sknmjJlyk3XXbBggWbNmqX+/ftr3rx5CgwM1MCBA3X06NFCqBwAAAAAgMLjXtQFXGvFihW6ePGiZs+eLX9/f0mSxWLRhAkTFBkZqbJly2a53pUrVzRv3jwNHDhQ/fv3lyQ1atRIDz74oBYtWqTx48cXzgMAAAAAAKAQONWR+S1btigsLMwe5CWpU6dOslqt2rZtW7br7du3TxcuXFCnTp3sbZ6enrr//vu1ZcuWgiwZAAAAAIBC51RhPi4uTkFBQQ5tJpNJgYGBiouLu+F6kjKtW716dZ04cUIpKSn5XywAAAAAAEXEqYbZm81mmUymTO1+fn5KSkq64Xqenp4qVqyYQ7vJZJLNZlNSUpK8vLxyXY/RaFBAgE+u17uTGQxX/z9+UJjSLdaiLeYaxTzdJEnlw/8jmyW9iKtxZPDwlCS93Pp5pVudq7Zibldru+dVJ3ze3K7uvvz8istmK+JicFtjv5Z77Nfyhv0aCgv7tdxjv5Y37Ndyz2g05HhZpwrzzsZgMMjNLedPJv7Hv0Sxmy9UBNx8/Iq6hGz5eZUo6hKy5envvM+b0ehUA4xwG2O/lnvs1/KG/RoKC/u13GO/ljfs1wqGUz2rJpNJycnJmdqTkpLk55f9m9NkMik1NVVXrlxxaDebzTIYDDdcFwAAAAAAV+NUYT4oKCjT3Pjk5GSdOXMm03z469eTpEOHDjm0x8XFqUKFCnkaYg8AAAAAgLNyqjDfunVrbd++XWaz2d62ceNGGY1GtWzZMtv1GjZsKF9fX3355Zf2trS0NG3evFmtW7cu0JoBAAAAAChsTjVnPjw8XDExMYqKilJkZKTi4+M1depUhYeHO1xjPiIiQidOnFBsbKwkqVixYoqMjFR0dLQCAgIUHBys5cuX6/z583rqqaeK6uEAAAAAAFAgnCrM+/n5aenSpZo4caKioqLk4+Oj7t27a8SIEQ7LWa1WWSwWh7ZBgwbJZrNp8eLFOnfunGrVqqVFixapcuXKhfkQAAAAAAAocAabjYsEAAAAAADgSpxqzjwAAAAAALg5wjwAAAAAAC6GMA8AAAAAgIshzAMAAAAA4GII8wAAAAAAuBjCPAAAAAAALoYwDwAAAACAiyHM444SHR2tkJAQ+39169ZVp06dtGDBAlmt1nzfXt++fRUZGZkvfZ07d05TpkxRx44dVbduXTVs2FB9+vTRp59+KovFIklatWqVQkJCdO7cuWz7ad++vV577TX77Yx1Mv5r3LixevTooa+++ipf6gbg6Pr9UMZ/nTt3liSFhIRo0aJFhVaP2WxWdHS0/vnnn0LbZm517dpVo0ePvulyNptNq1evVq9evdSoUSPVqVNHHTt21JQpUxQfH29f7mbPcXR0tBo0aODQdu1rldHvjBkzdOnSpbw/MCAba9euVffu3dWoUSM1bNhQnTp10iuvvKKEhIQC33Zu9kEHDhzQCy+8oFatWqlOnTpq0aKFhgwZoh07dtiXGT16tH3/lpVjx44pJCREGzdudFjn2s9cq1atNHjwYB04cOCG9WTsX++9994s/64LDw9XSEhIjvYnOfH666+rffv2uV7v+r/FsmOxWLRs2TI99thjqlevnho1aqSIiAh9//33eSlX0tXnaN++fZna8/O7J6ePLz/k9TW4XbgXdQFAYfPy8tLSpUslSSkpKdq1a5feeust2Ww2PfPMM/m6rVdffVVG463/Znb48GH169dPFotFAwYMUO3atZWamqqdO3dq8uTJKlmypO67775b2sbChQtVokQJnTt3Tu+//76ioqK0cOFC3XvvvbdcPwBH1+6Hrm2TpI8//lgVKlQotFrMZrNmz56tu+++WzVq1Ci07eY3m82mF154QV9++aW6deump59+Wr6+vvrnn3+0YsUKHT16VO++++4tbaNv377q3Lmzrly5ou3bt2vBggU6duyYZsyYkU+PApAWLFigt956S/3799fQoUNls9n0999/a926dTp9+rRKlSpV1CVKkr766iuNGDFCd999t0aMGKEqVaro3Llz2rx5swYOHKgff/xRJUqUyHP/lStX1vTp02Wz2XT48GHNmjVLffv21YYNGxQYGJjteh4eHkpMTNTu3bvVrFkze/vx48f1888/y9vbO881FSar1arnn39eW7ZsUd++ffXSSy/p8uXLWr16tZ555hmNGjVKAwcOzHW/s2fPlre3txo2bOjQnp/fPbNnz5bJZMqXvnBjhHnccYxGo+rXr2+/3bx5c/3111/avHlzvof5/PrD+MUXX5TFYtHKlStVtmxZe3vr1q3Vp08fJScn3/I2ateurYCAAElS06ZN1bZtWy1btowwDxSA6/dD18quHTf20UcfacOGDXr99dfVvXt3e3vTpk3Vo0cPbd269Za3Ub58efvr06xZM505c0YrV67U2LFj7ftP4FbFxMToscceczh63KZNGz399NMFMoowL86cOaNRo0apUaNGmj9/vjw9Pe33dezYUU888YTc3W8tZnh5edk/bw0aNFClSpXUu3dvrV27Vk899VS263l4eCgsLEwbNmxwCPMbNmzQ3XffnS8HWQrDsmXL9PXXX2vy5Mnq1q2bvb19+/YaNWqUpk+frrCwMNWqVStftpef3z333HNPvvWFG3ONdzNQwHx8fJSenu7QNn36dHXp0kUNGjTQvffeq5EjR+r06dMOy+zdu1e9e/dWo0aN1KBBA3Xp0kWrV6+235/VMPuDBw9qyJAhatq0qerVq6dHHnlE69evz7a2PXv26Ndff1VkZKRDkM9QoUIFhYSE5OVhZ8vX11fVqlXTsWPH8rVfADd3/VDHjP3Ixo0b1bFjRzVo0ED9+vXTkSNHHNZLTU3VjBkz1K5dO9WpU0edOnXSunXrbritY8eOqUOHDpKkYcOG2Ye0Hjt2TLt27VJISIh+++03h3Wee+459e3b1347Y0j6gQMH1LNnT9WrV0+dO3fWDz/8kGl7q1atUpcuXVS3bl3de++9mjlzpn2aUIZ9+/apW7duqlu3rjp37pzj4aTvv/++ateu7RDkM7i5ualNmzY56ic36tSpI0nsK5GvzGazypQpk+V9GUH03XffVdu2be3tVqtVjRs3VlhYmMPyrVq10sKFCyVJp0+f1pgxY9ShQweFhobqgQce0IwZM5SampppO1arVdHR0WrRooWaNWumMWPGOEwp+eSTT3ThwgWNGTPGIchnaN68uYoXL57rx34jufm8de7cWZs2bVJaWpq9bf369dkO99+9e7fCw8MVGhpqf7znz593WCY+Pl6DBw9WvXr1dO+992rBggVZ9nXq1Cm9+OKLatasmUJDQ9W7d2/9/vvvOXyU/7N06VJVq1ZNjz76aKb7hg4dKoPBoJiYGHtbxnfFmjVrdN999yk0NFR9+/ZVXFycfZmMvxenTp1q39/v2rXLfl9W3z3r16/XAw88oHr16mnw4MFKSkrS8ePH9dRTT6lBgwZ6+OGH7X1kuHaYfcY0iqz+W7VqlX2dn376Sf369VP9+vXVqFEjvfDCC5mmleT0NbiTcGQed6SM4J4xzH7z5s2ZQndCQoIiIyNVpkwZ+9DzjOFd7u7uunDhgiIjI9WoUSPNmDFDnp6e+ueff2Q2m7Pd7r///qsePXqofPnyeuWVVxQYGKi//vpLJ06cyHadH3/8UZIK9Qi5xWLRyZMndffddxfaNoE7zfU/ILq5uclgMGS57J9//qlz587ZR+lMmTJF//d//6ePP/7YvsywYcO0b98+RUVFqXr16vr+++/1f//3fzKZTNkG2TJlymj27NkaMmSIRo4caT+KVaZMGR0/fjzHjyUtLU0vvvii+vXrp+eee04LFizQ0KFD9c0336hkyZKSrobtadOmKSIiQqNHj9bBgwftYf7FF1+UdPVo31NPPaWQkBC9/fbbMpvNmjBhgi5dunTDo0+nTp3S0aNHNXjw4BzXnB8yQkVWP7QCeVW7dm2tWLFClSpVUtu2bbMcUt6kSRPNmjVLR48eVeXKlfXnn38qJSVFFy9e1MGDB1W9enUdOnRIZ86cUZMmTSRJiYmJ8vf315gxY2QymfTvv/8qOjpaZ86c0eTJkx36//DDD9WoUSNNmTJF//77r6ZOnapSpUrZP6u7d+9WmTJl8v1gwo1kfN6y+6HjWu3atdMrr7yibdu2qW3btvrnn3904MABvfvuu/riiy8clv399981YMAANWvWTO+8847Onj2rt956yz5Fx83NTdLVHzLj4+M1fvx4lShRQgsWLNDJkycdRiAkJSWpV69e8vb21n/+8x+VKFFCMTExioiI0ObNm3M8ReLkyZM6duyYBgwYkOVIgooVKyokJER79uxxaN+/f7+OHDmiF154QZL09ttv6+mnn9bGjRvl6empjz/+WD169LBPGZJuPIr0jz/+UGJiol566SVduHBBkyZN0n/+8x8dP35cjz76qAYMGKB58+bp+eef17fffisfH59MfZQpU8bhu0qSVq9erU8++UR33XWXpKtBvm/fvmrTpo1mzpypy5cv6+2339Zzzz3nsG5OXoM7zZ37yHHHunTpkmrXru3Q9tBDD2UaYn/tF5vFYlGDBg3UunVr7dy5U61atdKhQ4eUnJyskSNH2r/Mrv9F/HrR0dHy8PDQ8uXL5evrK0lq0aLFDdfJOGlTQc+htVqtSk9P17lz5zRnzhydOXNGzz//fIFuE7hTZbUfmjp1qrp27Zrl8snJyVqzZo19KPelS5c0ZswYnTp1SuXKldPOnTv1zTffaNGiRWrVqpUkqWXLljpz5oyio6OzDfOenp72kHzXXXfleZhlRpjP2E61atXUoUMHbdmyRV27dtWFCxc0a9YsPf300xo5cqS9Pg8PD02ZMkVPPfWUSpYsqaVLl8pgMGjBggX2ubblypVT//79b7j9jP1k+fLl81R/TmXsJ69cuaIdO3Zo+fLlatCgAWEe+erVV1/VkCFDNHbsWElSpUqV1K5dO/Xv31+VKlWSJNWrV0+enp7as2ePKleurN27d6tOnTq6cuWKfvzxR1WvXl179uyRt7e3fV8TEhKiUaNG2bfTsGFDFS9eXKNHj9a4ceMcjqQHBgbqrbfeknR1St8ff/yhTZs22cN8fHx8oZzbIz09XTabTUeOHNGrr74qDw8P+2iiGylevLjat2+vDRs2qG3btlq/fr0aNGigypUrZ1p27ty5CgwM1Ny5c+Xh4SHp6r7kqaee0vfff6/27dtry5Yt+v3337VkyRL733rNmjVTmzZt5O/vb+9r6dKlMpvN+vTTT+3BPSwsTB07dtSiRYv00ksv5ehx52SfVr58+UwjoBISErRs2TJVrVpV0tXh7g8++KBWrVql8PBw+z7+2ilDN3LhwgXNnTvX/t1z4MABLV68WOPHj1fPnj0lXQ3rXbp00Y4dO7I8f5Onp6fDtvbt26eVK1dq6NChatSokSTprbfeUp06dTR79mz7j9rBwcH20Vlt2rTJ8WtwpyHM447j5eWlZcuWSbo6LHX//v2aNWuWxo4d6xDgv//+e82ZM0d///23Lly4YG//999/1apVK1WpUkW+vr4aP368+vbtq+bNm990zuTOnTvVsWNHe5B3Ji1btrT/28vLS88++6yefPLJIqwIuH1dux/KkNUfmRlq1qzpsH/JOJKSEea3bdsmf39/NW/e3OGIf4sWLTR+/HhZLBYZjUaHIe0Gg8F+xOlWGY1Ghx8zK1WqJC8vL/sfpD/99JMuXbqkBx98MFN9KSkp+vvvv9W0aVP98ssvatasmcNJs8LCwnL8h1p2Ixvyy/Tp0zV9+nT77ZYtWxbaGZtx5wgODtb69eu1Y8cObd26Vbt371ZMTIxWrVqlDz/8ULVq1VKxYsVUt25d7d69W4899pj27Nmjpk2bKiUlRbt371bPnj21e/du1a9f337U0mazaenSpfrkk0907NgxXblyxb7No0ePKjg42H77+gMN1atX14YNGxzaCvrz9vfffzv86FmmTBlNmzbNoc4b6dy5s1544QWlpKToiy++cJgedK09e/aoc+fO9iAvXZ2eYDKZtHfvXrVv316//vqrSpQo4bCfK1GihFq0aKE//vjD3rZt2zY1a9ZMfn5+9n2d0WhUkyZNMk1ZKgh33323PchLV3+krVmzpn755ReFh4fnur/rv3sy+r72/ZHRdurUqZv2d+rUKT3//PNq3769nn32WUnS5cuXtW/fPr300ksO31FVq1ZV+fLl9dtvv6lNmzY5fg3uNIR53HGMRqPq1q1rv92oUSP7sNUBAwYoODhYv/76q5577jl16NBBgwYNUqlSpWQwGPTkk0/av/z8/Pz0/vvva9asWfYdUOPGjTV27Nhsh52dP38+R8PDrpVxxOfkyZP24UgFYcmSJfL19ZWfn58qVKhwRw9ZAgra9fuhm7n+rMAZf3Rm7I8SExN1/vz5TEf7M5w5c8Z+VYwMTZs2dZhveSu8vLwyzZv18PBwqE+SHnvssSzXP3nypL3OrPZzN/uhNGM/eaMpS/mhX79+euSRR+Tp6amKFSs65Q+zuD14enqqTZs29tEuP/zwgyIjI/Xuu+9q9uzZkq4Otf/yyy8lXT2HT8bfKBk/MO3Zs0ePP/64vc+lS5fqzTff1NNPP61mzZrJZDLpt99+02uvveYQ7KWs9znXzq0vW7asw1zsglClShXNmDFDBoNBgYGBKlOmTK5+QGjVqpU8PDz0zjvv6NixY+rUqVOWy5nN5iyHv5cqVUpJSUmSrp5vIKv90PXrJSYm6ueff85yX1ylSpUc137t337ZOXnypMqVK3fDejLazpw5k+NtXyu7755rf3DN2Pdf/x66XkpKip577jkFBARoypQp9naz2SyLxaLJkydnmu4h/e85yOlrcKfhr3VAUlBQkCTpn3/+UXBwsL766iv5+vrq7bffts9Vymr+aGhoqBYuXGife//mm28qKioq22u0+/v7ZzqJ3s00bdpUkrR169YCDfMhISGcjRlwUX5+fgoICND8+fOzvD8gIEC+vr767LPP7G1ZzW28VrFixSTJ4QRS0tU/vHJ7RM7Pz0/S1csVXf/HpyT70OHAwMAsr6N97ty5G/Zfrlw5ValSRVu3btWIESNyVVtulCtXLlc/wgD55d5771XNmjV18OBBe1uTJk00d+5c7dixQ0lJSWrYsKFSU1N15swZ7dixQ8ePH1fjxo3ty2/cuFHt27e3z6eW5NBfbjRt2lQ7duzQ33//XWDn18kYfZBXHh4eeuCBB+zDskuXLp3lcn5+flnudxISEuz7rozzJ2W1zPV93XvvvRo2bFimZbM6UWB2ypcvr0qVKmnLli0aNWpUpn3uiRMndODAgUxTs7J7HDVr1szxtgvKK6+8omPHjumzzz5zuDxgiRIlZDAYFBkZmeUw/YzzruT0NbjTcDZ7QFeHckn/22GkpKTIw8PDYed5o7NCe3l5qU2bNurZs2emoWvXCgsL06ZNmxyG7d9M48aNFRoaqrlz52b5Q8DJkyd14MCBHPcH4PbTokULnTt3Th4eHqpbt26m/zw9PeXr6+vQlvEj5vVH+TNkhO5r/9g/d+6c9u/fn+v6GjRooOLFi+vUqVNZ1pex7w0NDdWuXbscLre5Y8eOTGeVzkr//v31+++/O1xRJIPVatWWLVtyXTdQFM6ePZupLSUlRSdPnnQIpA0aNJC7u7vee+891apVS76+vgoICFCNGjX03nvvycPDw2GucsbfNte62RUvsvPEE0/I19dXkydPzvSDnyTt2rVLly9fzlPf+emJJ55Qu3btHEYlXa9Ro0b6+uuvHaYAbdu2TWaz2T6nu27dukpOTtaOHTvsyyQnJ2v79u0OfbVo0cJ+AsLr93O5PVlgRESEDh48qM8//zzTfdHR0bLZbJmmDvz99986fPiw/fbhw4f13//+V/Xq1bO3XTtqqrDMnz9fX375pWbMmJFphIK3t7fq16+vuLi4LL8fMn7szelrcKfhyDzuOFarVT///LOkq0ec9u/frzlz5qhGjRr2X7BbtmyppUuXauLEibr//vv1008/ZdqZfvfdd/rss8903333qUKFCjp79qyWLVumhg0b2o9oXW/IkCH67rvv1KtXLz399NMKDAzUwYMHdfnyZQ0aNCjbmqdPn66+ffvq8ccf14ABA1S7dm2lpqZq9+7d+vDDD/Xmm286fElkdUbRu+++W9WrV8/LUwbAybVs2VLt2rXT008/raefflohISG6fPmy/vnnHx0+fFivv/56tusGBgbKZDJpw4YNqlSpkjw9PRUSEqJy5cqpXr16evfdd1WiRAm5u7s7nJguN0wmk4YOHapp06bp1KlTatq0qdzc3HT06FF9/fXXio6OVvHixRUREaGPPvpIgwYN0qBBg2Q2mxUdHZ2jOfO9evXS3r179corr2jfvn3q0KGDvL29FRcXpxUrVqhixYpq3bq1ffm//vpLGzdudOjD29vbYRmgKHTp0kXt2rVTq1atVKZMGcXHx2vZsmVKTExURESEfTkfHx/VqlVLP/74owYMGGBvb9y4sZYvX57p75EWLVrogw8+sJ8gbe3atQ7BLzcCAwP15ptvavjw4erZs6d69+6typUrKzExUV999ZXWrVvncLmyCxcuZPq8SXK4DnxBCA0N1XvvvXfDZQYPHqzw8HBFRkaqb9++9rPZh4aG2qc5tG7dWrVr19b//d//6cUXX1SJEiU0f/78TFNt+vfvr3Xr1qlPnz7q16+fKlSooHPnzumXX35R2bJlb3oyz2v16dNHO3fu1NixY3XgwAG1bt1aV65c0apVq7Rp0yaNGjUq01U+SpUqpcGDB2vo0KGSpHfeeUdly5Z1uE59UFCQvv76azVu3FjFixdXtWrVCnTK0N69ezVz5kw99NBD8vX1tf8NLl2dehAQEKCXXnpJERERGj58uB5++GGZTCadOnVK27dvV7du3dSsWbMcvwZ3GsI87jgpKSnq0aOHJMnd3V3lypXTI488oiFDhth/sW7Tpo1efPFFLVu2TKtWrVLDhg01b948dezY0d5PlSpVZDQa9fbbbyshIUH+/v5q1aqV/UzNWalatapWrFiht956SxMmTJDFYlHVqlUznUn/enfddZdWr16tBQsWaPny5Tp58qQ8PT11zz336OWXX1a7du0cln/55Zcz9TFs2DA999xzOX6eALiWWbNmaf78+Vq+fLmOHz+uEiVK6O6773b4Iy4rRqNRkydP1owZM9S/f3+lpqbq66+/VqVKlTR9+nSNHTtWY8aMUenSpTV8+HBt2LDB4ch5Tg0cOFBly5bV+++/r2XLlsnd3V1VqlRR27Zt7fveMmXKaMGCBZo0aZKGDRumKlWqaNy4cZo5c+ZN+zcYDHrrrbfUqlUrffrpp9qwYYNSU1NVsWJFtW/fXgMHDnRYfs2aNVqzZo1DW5UqVRQbG5vrxwbkpyFDhujbb7/VlClTdO7cOZUsWVIhISFasmSJmjdv7rBsxonVrh1O37RpUy1fvtx+SboMUVFRSkxM1KxZsyRJHTt21NixY/N8Scf77rtPn332mRYsWKC33npLiYmJMplMatSokRYvXuzww9/JkyezHHr+4YcfZjn1pjDVqVNHixcv1owZM/T888/L29tb7du316hRo+wnCTUYDHrvvff06quvaty4cTKZTPbg//XXX9v7KlmypD7++GO9/fbbmj59us6fP69SpUqpXr16uv/++3NVl9FoVHR0tD766COtXLlSH330kdzd3VW7dm3Nnz8/y6uU1K5dWw888ICmTZumM2fOqF69epowYYLDEP9x48bpjTfe0KBBg5SSkqIPPvigQH9UOXz4sKxWq9avX6/169c73Dd58mR169ZNDRs21EcffaTo6GiNGTNGaWlpKleunJo3b26fYprT1+BOY7DZbLaiLgIAAAAAkDd9+/aVt7e35s2bV9SloBAxZx4AAAAAABdDmAcAAAAAwMUwzB4AAAAAABfDkXkAAAAAAFwMYR4AAAAAABdDmAcAAAAAwMUQ5gEAAAAAcDGEeQAAUKhCQkIUHR2d6/WOHTumkJAQrVq1qgCqAgDAtRDmAQC4Q61atUohISEKCQnRnj17Mt1vs9nUpk0bhYSEKDIysggqBAAA2SHMAwBwhytWrJjWr1+fqf3HH3/UqVOn5OnpWQRVAQCAGyHMAwBwh2vTpo02btyo9PR0h/b169erdu3aCgwMLKLKAABAdgjzAADc4R5++GGdP39e27Zts7f9v/buL6SpNoDj+Fd974yoEXUx6KIMEbzIICyJCYMY5QyCEmqE4UUUEV0UQUnURUgEMegPRhD0hwopCrQCq1EtwtlVN0J0ESQICTKojTBhO++Vexn6QrzkG0e+HxjjnPNjz/Ps7sdzzjYzM8Pw8DCdnZ1z8j9+/OD8+fO0t7fT3NxMIpHgxo0bBEFQlZuZmaGvr49NmzbR0tLCwYMH+fr167xzmJyc5OTJk7S1tdHc3ExHRwcPHz78vQuVJGkR+etPT0CSJP1Z0WiU9evX8/TpU9rb2wHIZrMUCgW2b9/OnTt3KtkgCDh06BCjo6Ps2rWLpqYm3r59y4ULF5icnOTUqVOVbG9vL4ODgySTSTZs2EAul+PAgQNzxp+amqKrq4uamhpSqRSRSIRsNktvby/FYpH9+/cv+HcgSVLYuDMvSZLo7Ozk5cuXTE9PAzA0NMTGjRtZtWpVVS6TyZDL5Th69Cjnzp0jlUpx7do1EokEt2/fZnx8HICPHz8yODjI3r17uXjxIqlUisuXL7Nu3bo5Y6fTaUqlEo8fP+bw4cPs2bOH/v5+Ojo6uHLlSmVOkiTpH5Z5SZLEtm3b+PnzJ69evaJYLPL69et5b7HPZrPU1dWxb9++qvM9PT0EQUA2mwXgzZs3AHNy3d3dVcdBEPD8+XPi8ThBEJDP5yuvLVu2UCgUGBsb+51LlSRpUfA2e0mSRCQSYfPmzTx58oTp6WlKpRKJRGJObmJigpUrV7JkyZKq82vXrq1cn32vra1l9erVVbk1a9ZUHefzeb5//87AwAADAwPzzi2fz//ndUmStFhZ5iVJEgDJZJLTp08zNTVFLBZj6dKlCz5muVwGYMeOHezcuXPeTGNj44LPQ5KksLHMS5IkALZu3cqZM2f48OED6XR63kw0GmVkZIRisVi1O//58+fK9dn3crnM+Ph41W78bG5WJBKhvr6ecrlMW1vb716SJEmLls/MS5IkAOrr6zl79ixHjhwhHo/Pm4nFYpRKJe7evVt1/ubNm9TU1BCLxSo5oOqX8AFu3bpVdVxXV0cikWB4eJhPnz7NGc9b7CVJmp8785IkqeLfbnWfFY/HaW1tJZ1OMzExQWNjI+/evSOTydDd3V15Rr6pqYlkMsm9e/coFAq0tLSQy+X48uXLnM88duwYo6OjdHV1sXv3bhoaGvj27RtjY2OMjIzw/v37BVmrJElhZpmXJEm/rLa2lv7+fi5dusSzZ8949OgR0WiUEydO0NPTU5Xt6+tj+fLlDA0NkclkaG1t5fr165X/sp+1YsUKHjx4wNWrV3nx4gX3799n2bJlNDQ0cPz48f9zeZIkhUZNEATBn56EJEmSJEn6dT4zL0mSJElSyFjmJUmSJEkKGcu8JEmSJEkhY5mXJEmSJClkLPOSJEmSJIWMZV6SJEmSpJCxzEuSJEmSFDKWeUmSJEmSQsYyL0mSJElSyFjmJUmSJEkKGcu8JEmSJEkhY5mXJEmSJClkLPOSJEmSJIXM34ZdXcgHNBkKAAAAAElFTkSuQmCC"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"results_df.to_csv('/kaggle/working/results_df.csv')","metadata":{"execution":{"iopub.status.busy":"2024-11-25T20:28:29.027379Z","iopub.execute_input":"2024-11-25T20:28:29.028097Z","iopub.status.idle":"2024-11-25T20:28:29.037501Z","shell.execute_reply.started":"2024-11-25T20:28:29.028065Z","shell.execute_reply":"2024-11-25T20:28:29.036451Z"},"trusted":true},"outputs":[],"execution_count":37}]}